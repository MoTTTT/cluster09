{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cluster09, a PodZone project","text":"<p>This repo contains a bootstraping and scaffolding and configuration for</p> <ul> <li>A Cluster API management cluster</li> <li>Cluster API template for a kubernetes cluster</li> <li>Template input to instantiate a managed cluster (Cluster09)</li> <li>Cluster09 gitops infrastructure configuration (Storage, Networking, Gateway)</li> <li>Cluster09 gitops workload configuration (Visibility tools)</li> <li>Cluster09 gitops authC authZ services (Fine grained and delegated access control</li> </ul>"},{"location":"#cluster-api-management-cluster","title":"Cluster API management cluster","text":"<ul> <li>Scripts to substitute instance variables into templates, for the bootstrapping of a Cluster API management cluster, and providing notes on user driven configuration.</li> <li>Templates generated Teraform VM definitions</li> <li>Templates for generated Talos configuration</li> <li>Script for Talos configuration form generated configurations</li> </ul>"},{"location":"#technology-considerations","title":"Technology considerations","text":""},{"location":"#specification","title":"Specification","text":"<ul> <li>Automated, repeatable cluster provisioning</li> <li>Provisioning scaffolding, (clean slate environments)</li> <li>Isolated provisioning (airgapped environment bootstrap)</li> </ul>"},{"location":"#dependency-management","title":"Dependency management","text":"<p>To support airgapped green-fields deployment, all dependencies need to be met locally. To leverage automated deployments, the following cached (and therefore version controlled) information is required.</p> <ul> <li>Local image mirror</li> <li>local config repo, with documentation, configuration, and scripts</li> <li>manifest repo, with cluster09 cluster and workload manifests</li> <li>tested with network airgapped bootstrap to workload commissioned</li> <li>Type approved bastion or bootstrap server configuration and build notes</li> </ul>"},{"location":"#capi-process-component-and-workflow","title":"CAPI: Process, Component, and Workflow","text":"<ul> <li>Talos for CAPI management cluster</li> <li>Terraform for CAPI cluster VMs</li> <li>Bash for CAPI Terraform and Talos template substitution</li> <li>Manual Terraform execution</li> <li>Scripted Talos Installation for CAPI</li> <li>Manual Talos cluster bootstrapping</li> <li>Manual access configuration extraction</li> <li>Manual gitops CAPI cluster workload bootstrapping (for future cluster API operator)</li> <li>Manual clusterctl CAPI cluster workload bootstrapping </li> </ul>"},{"location":"#cluster09-process-components-and-workflow","title":"Cluster09: Process, Components, and Workflow","text":"<ul> <li>Cluster template for proxmox and talos provided cluster</li> <li>Manual cluster configuration</li> <li>clustercli manifest generation from template, and configuration</li> <li>gitops  </li> </ul>"},{"location":"#platform-process-components-and-workflow","title":"Platform: Process, Components, and Workflow","text":"<ul> <li>Proxmox hypervisor</li> <li>Talos for management and provisioned clusters (Cluster09)</li> </ul>"},{"location":"#proxmox-hypervisor","title":"Proxmox hypervisor","text":""},{"location":"#-proxmox-v9-installed","title":"- Proxmox V9 installed","text":""},{"location":"Cilium/","title":"Cilium","text":""},{"location":"Cilium/#generating-the-cilium-manifest","title":"Generating the cilium manifest","text":"<ul> <li><code>helm repo add cilium https://helm.cilium.io/</code></li> <li><code>helm repo update</code></li> </ul> <pre><code>helm template \\\n    cilium \\\n    cilium/cilium \\\n    --version 1.17.4 \\\n    --set hubble.relay.enabled=true \\\n    --set hubble.ui.enabled=true \\\n    --set ingressController.enabled=true \\\n    --set ingressController.loadbalancerMode=shared \\\n    --set ingressController.default=true \\\n    --set l2announcements.enabled=true \\\n    --set l2announcements.leaseDuration=3s \\\n    --set l2announcements.leaseRenewDeadline=1s \\\n    --set l2announcements.leaseRetryPeriod=200ms \\\n    --set loadBalancerIPs.enable=true \\\n    --set gatewayAPI.enabled=true \\\n    --set loadBalancer.l7.backend=envoy \\\n    --namespace kube-system \\\n    --set ipam.mode=kubernetes \\\n    --set kubeProxyReplacement=true \\\n    --set securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n    --set securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n    --set cgroup.autoMount.enabled=false \\\n    --set cgroup.hostRoot=/sys/fs/cgroup \\\n    --set k8sServiceHost=localhost \\\n    --set k8sServicePort=7445 &gt; cilium.yaml\n</code></pre> <p>This enables:</p> <ul> <li>hubble, and hubble ui</li> <li>ingres controller, set to shared loadbalancer</li> <li>l2 announcements on loadbalancer IPs</li> <li>gateway api</li> </ul> <p>Add IP pool:</p> <pre><code>apiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumLoadBalancerIPPool\nmetadata:\n  name: \"ip-pool\"\nspec:\n  blocks:\n  - start: \"192.168.4.90\"\n    stop: \"192.168.4.90\"\n</code></pre> <p>Add L2 announcement policy:</p> <pre><code>apiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumL2AnnouncementPolicy\nmetadata:\n  name: l2policy\nspec:\n  loadBalancerIPs: true\n</code></pre>"},{"location":"Cilium/#gateway","title":"Gateway","text":"<p>Example GatewayClass and Gateway, with TLS termination using cert-manager:</p> <pre><code>---\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: GatewayClass\nmetadata:\n  name: cilium\nspec:\n  controllerName: io.cilium/gateway-controller\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: cluster08\n  namespace: kube-system\n  annotations:\n    cert-manager.io/cluster-issuer: lets-encrypt\nspec:\n  gatewayClassName: cilium\n  listeners:\n  - hostname: cluster08.podzone.cloud\n    name: cluster08-podzone-cloud-http\n    port: 80\n    protocol: HTTP\n  - hostname: cluster08.podzone.cloud\n    name: cluster08-podzone-cloud-https\n    port: 443\n    protocol: HTTPS\n    tls:\n      mode: Terminate\n      certificateRefs:\n        - name: cluster08-secret\n    allowedRoutes:\n      namespaces:\n        from: All\n</code></pre>"},{"location":"Cilium/#implementation-notes","title":"Implementation notes","text":""},{"location":"Cilium/#v1-iteration","title":"V1 Iteration","text":"<ul> <li>Install on talos</li> <li>Generate cilium manifest:</li> <li>For L2 announcements, and cilium ingress controller, set:</li> <li>externalIPs.enabled=true</li> <li>l2announcements.enabled=true</li> <li>ingressController.enabled=true</li> <li>ingressController.loadbalancerMode=shared</li> <li>kubeProxyReplacement=true</li> <li>l7Proxy=true</li> <li>envoyConfig.enabled=true</li> <li>loadBalancer.l7.backend=envoy</li> <li>ingressController.default=true</li> </ul>"},{"location":"Cilium/#v2-installation","title":"V2 installation","text":"<ul> <li>Include GatewayAPI</li> <li>Enable Hubble relay and UI</li> <li>L2 announcement lease config</li> <li>Enable loadBalancerIPs</li> <li>Add to talos controlplane-patch.yaml</li> <li>Create patch template with {proxy,disable cni, disable proxy, asocp, stub for cilium manifest}</li> <li>controlplane-patch-template.yaml</li> </ul>"},{"location":"Cilium/#v3-installation","title":"V3 installation","text":"<ul> <li>Generate cilium manifest, as above, and serve from URL, add to extraManifests in talos config.</li> </ul>"},{"location":"Cilium/#issues","title":"Issues","text":"<ul> <li>Installation of opensearch: <code>invalid: metadata.labels: Invalid value: \\\"opensearch-opensearch-dashboard-opensearch-dashboards-dashboards\\\": must be no more than 63 characters\"</code></li> </ul>"},{"location":"Cilium/#references","title":"References","text":"<ul> <li>https://docs.cilium.io/en/latest/network/servicemesh/tls-termination/</li> <li>https://cilium.io/use-cases/gateway-api/</li> <li>https://docs.cilium.io/en/stable/network/lb-ipam/</li> <li>https://isovalent.com/blog/post/migrating-from-metallb-to-cilium/</li> <li>https://docs.cilium.io/en/stable/network/servicemesh/ingress/</li> <li>https://gateway-api.sigs.k8s.io/</li> <li>https://docs.cilium.io/en/v1.14/network/servicemesh/gateway-api/gateway-api/</li> <li>https://itnext.io/cilium-gateway-api-cert-manager-and-lets-encrypt-updates-cc730818cb17</li> <li>https://blog.grosdouli.dev/blog/cilium-gateway-api-cert-manager-let's-encrypt</li> <li>https://github.com/cilium/cilium/blob/v1.18.1/install/kubernetes/cilium/values.yaml#L992</li> <li>https://cert-manager.io/docs/usage/gateway/</li> </ul>"},{"location":"ClusterAPI/","title":"Cluster API","text":""},{"location":"ClusterAPI/#clusterctl","title":"clusterctl","text":"<ul> <li>Tested successfully with 2 node talos cluster (Aug 2025)</li> <li>Install cert-manager: <code>helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true</code></li> <li>Set up clusterctl config file: <code>~/.cluster-api/clusterctl.yaml</code> (On initial install only CAPI mandatory fields are set)</li> <li>Install components: <code>clusterctl init --ipam in-cluster --core cluster-api -c talos -b talos -i proxmox</code></li> <li>Version installed, based on init response:</li> </ul> <pre><code>Installing provider=\"cluster-api\" version=\"v1.10.5\" targetNamespace=\"capi-system\"\nInstalling provider=\"bootstrap-talos\" version=\"v0.6.9\" targetNamespace=\"cabpt-system\"\nInstalling provider=\"control-plane-talos\" version=\"v0.5.10\" targetNamespace=\"cacppt-system\"\nInstalling provider=\"infrastructure-proxmox\" version=\"v0.7.3\" targetNamespace=\"capmox-system\"\nInstalling provider=\"ipam-in-cluster\" version=\"v1.0.3\" targetNamespace=\"capi-ipam-in-cluster-system\"\n</code></pre> <p>clusterctl generate yaml --list-variables --from cluster-template.yaml &gt; cluster08-variables.yaml clusterctl generate yaml --from cluster-template.yaml &gt; cluster08.yaml </p> <ul> <li>Spin up a cluster: <code>kubectl apply -f proxmoxcluster.yaml</code></li> <li>Describe cluster: <code>clusterctl describe cluster cluster07 -n cluster07 --show-conditions all --show-templates --show-resourcesets --grouping=false --echo</code></li> <li>Retrieve talos config: <code>kubectl get secret --namespace cluster07 cluster07-talosconfig -o jsonpath='{.data.talosconfig}' | base64 -d &gt; cluster07-talosconfig</code></li> <li>Retrieve kubeconfig: <code>talosctl kubeconfig --nodes 192.168.4.170 --endpoints 192.168.4.170 --talosconfig=./cluster07-talosconfig</code></li> <li>To retrieve file: <code>kubectl get secret --namespace cluster07 cluster07-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; cluster07-kubeconfig</code></li> <li>scale up cluster control plane: Change replicas to 3</li> <li>scale up workers: Change replicas to 2</li> <li>bootstrap flux: <code>flux bootstrap github --context=admin@cluster07 --owner=MoTTTT --repository=venus --branch=main --personal --path=clusters/managedcluster07 --token-auth=true</code></li> </ul>"},{"location":"ClusterAPI/#cluster-api-operator","title":"Cluster API Operator","text":"<ul> <li>Install cert-manager: <code>helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true</code></li> <li>Install CAPI Operator: <code>helm install capi-operator capi-operator/cluster-api-operator --create-namespace -n capi-operator-system --set infrastructure.proxmox.enabled=true --set controlPlane.talos.enabled=true --set  bootstrap.talos.enabled=true --wait --timeout 90s</code></li> <li>Configure proxmox credentials: <code>kubectl apply -f capmox-manager-credentials.yml</code></li> <li><code>helm repo add capi-operator https://kubernetes-sigs.github.io/cluster-api-operator</code></li> <li><code>helm install capi-operator capi-operator/cluster-api-operator --create-namespace -n capi-operator-system --set infrastructure.proxmox.enabled=true --set configSecret.name=capmox-manager-credentials --set configSecret.namespace=default --set controlPlane.talos.enabled=true --set  bootstrap.talos.enabled=true --wait --timeout 90s</code></li> </ul>"},{"location":"ClusterAPI/#issues-and-notes","title":"Issues and notes","text":"<ul> <li>Setting IP addresses using cloudinit: <code>You will need to make sure that your VM template doesn't have Cloud-init Driver provided by Proxmox, otherwise, that will overwrite the config of CAPMOX. No need to pre-set up the Cloud-init Drive.Just use an empty CD ROM at ide0, and CAPMOX will do the job.</code></li> <li><code>clusterctl generate cluster</code> does not have an option to specify control-plane or bootstrap providers, and assumes cubeadm for both.</li> <li>clusterctl init creates a secret called <code>capmox-manager-credentials</code> in the <code>capmox-system</code> namespace.</li> <li>cluster operator creates this in the <code>proxmox-infrastructure-system namespace</code> (with empty values).</li> <li>Loading a secret manifest sets the values. Loading a cluster manifest resets them to their defaults (to be confirmed...)</li> <li>In cluster operator scenario, Proxmox expects ProxmoxCluster to include a spec.credentialsRef.</li> <li><code>clusterctl generate cluster</code> does not include spec.credentialsRef in the generated ProxmoxCluster manifest.</li> <li>The above discrepancies could relate to version differences.</li> <li>Generate cluster manifest: <code>clusterctl generate cluster cluster07 --kubernetes-version v1.31.2 --control-plane-machine-count=1 --infrastructure=proxmox:v0.7.3 &gt; cluster07.yaml</code></li> <li>NOTE: This is not useful for talos controlplane an bootstrap unless a custom template is used.</li> <li>For cluster management investigate headlamp, https://github.com/Jont828/cluster-api-visualizer/tree/main</li> </ul>"},{"location":"ClusterAPI/#references","title":"References","text":"<ul> <li>https://cluster-api.sigs.k8s.io/user/quick-start</li> <li>https://github.com/k8s-proxmox/cluster-api-provider-proxmox</li> <li>https://github.com/siderolabs/cluster-api-bootstrap-provider-talos</li> <li>https://github.com/siderolabs/cluster-api-control-plane-provider-talos</li> <li>Discussion on flux with Cluster API Operator: https://github.com/ionos-cloud/cluster-api-provider-proxmox/issues/221</li> <li>Flux repo example: https://github.com/a7d-corp/homelab-clusters-fleet/tree/main</li> </ul>"},{"location":"ClusterTaskTracking/","title":"Talos","text":""},{"location":"ClusterTaskTracking/#log-visibility","title":"Log Visibility","text":""},{"location":"ClusterTaskTracking/#talos-system-level-interrogation","title":"Talos system level interrogation","text":"<ul> <li>System logs and status logs: <code>talosctl -n 192.168.4.121 dmesg</code>, <code>talosctl -n 192.168.4.121 services</code>, <code>talosctl -n 192.168.4.121 logs machined</code></li> <li>Container list: <code>talosctl -n 192.168.4.121 containers -k</code></li> <li>Container logs: <code>talosctl -n 192.168.4.121 logs -k kube-system/cilium-cxrpb:cilium-agent:904e8a41ccff</code></li> </ul>"},{"location":"ClusterTaskTracking/#aggregating-logs","title":"Aggregating logs","text":"<ul> <li>Using opensearch for search index, and opensearch-dashboard for visibility</li> <li>Using fluentbit for log collection</li> </ul>"},{"location":"ClusterTaskTracking/#generation-2-configuration","title":"Generation  2 Configuration","text":""},{"location":"ClusterTaskTracking/#backlog","title":"Backlog","text":"<ul> <li>[ ] Move Loadbalancer manifest to cluster config</li> <li>[ ] Log visibility: Send service and kernel logs to fluentbit: Set fluentbit service up as a NodePort</li> <li>[ ] Managed secrets: <code>talosctl gen secrets --from-controlplane-config controlplane.yaml -o secrets.yaml</code></li> <li>[ ] Observability: tracing with Jaeger</li> <li>[ ] Observability / Security: Falco (Security monitoring)</li> <li>[ ] Security: Kyverno (Policy as code)</li> <li>[ ] Security: Keycloak</li> <li>[ ] OpenTelemetry</li> <li>[ ] Evaluate TALM: <code>https://github.com/cozystack/talm</code></li> <li>[ ] Evaluate authentik</li> <li>[ ] Automated provisioning: <code>https://cozystack.io/</code></li> <li>[ ] Switch to Gateway API: https://medium.com/@martin.hodges/why-do-i-need-an-api-gateway-on-a-kubernetes-cluster-c70f15da836c</li> <li>[ ] Split out storage network: <code>https://cozystack.io/docs/operations/storage/dedicated-network/</code></li> <li>[ ] Refactor ingresses with URL prefixes</li> <li>[ ] Security SSO: Oauth2-proxy</li> <li>[ ] SOPS with age https://fluxcd.io/flux/guides/mozilla-sops/#encrypting-secrets-using-age; https://pkg.go.dev/filippo.io/age</li> <li>[ ] Investigate kubewall https://github.com/kubewall/kubewall</li> <li>[ ] Investigate Kubeshark: Network traffic analyser https://github.com/kubeshark/kubeshark</li> <li>[ ] Investigate Pixie https://px.dev/</li> <li>[ ] Investigate Jaeger https://github.com/jaegertracing/jaeger-operator</li> <li>[ ] Investigate OpenTelemetry: https://opentelemetry.io/; https://github.com/open-telemetry/opentelemetry-operator</li> <li>[ ] Investigate Kubeflow (AI Tool ecosystem): https://www.kubeflow.org/</li> <li>[ ] Investigate: For VMs in k8s, see kubevirt</li> <li>[ ] Investigate: For flux git access secret https://fluxcd.io/flux/cmd/flux_create_secret_git/</li> </ul>"},{"location":"ClusterTaskTracking/#cluster08-changes","title":"Cluster08 changes","text":"<ul> <li>[X] Template for <code>clusterctl generate cluster</code>: <code>clusterctl generate yaml  --from cluster-template.yaml &gt; cluster08.yaml</code></li> <li>[ ] Gateway API for Hubble: https://blog.grosdouli.dev/blog/cilium-gateway-api-cert-manager-let's-encrypt</li> </ul>"},{"location":"ClusterTaskTracking/#cluster07-changes","title":"Cluster07 changes","text":"<ul> <li>[X] Cluster API: clusterctl for provisioning (cluster api operator rolled back)</li> <li>[X] Final talos config {VIP, mirror registry (harbor), drbd, sysctls, certSANs, cilium, talos-cloud-controller-manager}</li> <li>[X] Move extraManifests to local httpd {kubelet-serving-cert-approver, metrics-server, piraeus-operator, gateway-api, cilium}</li> <li>[X] Reintroduce support for cilium, drdb</li> <li>[ ] Fix mv naming (what was this?)</li> </ul>"},{"location":"ClusterTaskTracking/#cluster06-changes","title":"Cluster06 changes","text":"<ul> <li>[X] Switch to Cluster API for provisioning, with simplified talos config</li> </ul>"},{"location":"ClusterTaskTracking/#cluster05-changes","title":"Cluster05 changes","text":"<ul> <li>[X] Use TALM for talos configuration: Cancelled</li> <li>[X] Reduce disk to 40 GB: Reduce storage startup time?: {8 CPU; 16 GB RAM; 40 GB Disk}</li> </ul>"},{"location":"ClusterTaskTracking/#cluster04-changes","title":"Cluster04 changes","text":"<ul> <li>[X] Dependency: Harbor</li> <li>[X] Configure machine.registry (Harbour) as docker caching repository</li> <li>[X] Workload: Prometheus and Grafana</li> <li>[X] Cache Talos startup image on bastion server: <code>https://factory.talos.dev/image/ed7716909fb764e0c322ab43dd20918e30cf8ffa3914ba3fa229afec9efe4d84/v1.10.2/nocloud-amd64.iso</code></li> <li>[X] Helm chart dependsOn: Fix for dashboards index creation failure</li> <li>[X] Split Opensearch roles</li> <li>[X] Distribute ingresses</li> <li>[X] Add worker node</li> <li>[X] Workload: Keycloak</li> </ul>"},{"location":"ClusterTaskTracking/#cluster03-changes","title":"Cluster03 Changes","text":"<ul> <li>[X] Workload: Radio station (ingress, nfs, storageclass)</li> <li>[X] Dependency: NFS</li> <li>[X] Add log visibility: Machine definition pre-requisites</li> <li>[X] Increase boot size to accommodate linstore pool usage</li> <li>[X] VM dimensions {8 CPU; 16 GB RAM; 80 GB Disk}</li> </ul>"},{"location":"ClusterTaskTracking/#cluster02-changes","title":"Cluster02 Changes","text":"<ul> <li>[X] Workload: OpenSearch with Dashboard</li> <li>[X] Workload: fluentbit {collect logs from kubernetes containers}</li> <li>[X] Move affinity controller installation to after linstor is stable</li> <li>[X] Fix: Shared ingress</li> <li>[X] Adjust resource allocations</li> <li>[X] Abstraction of controlplane and worker resource dimensions</li> <li>[X] Abstraction of Talos version</li> <li>[X] Kustomization patch for Ingres load balancer IP Pool</li> <li>[X] Support for internet proxy:</li> <li>[X] VM dimensions {8 CPU; 16 GB RAM; 40 GB Disk}</li> </ul>"},{"location":"ClusterTaskTracking/#generation-01","title":"Generation 01","text":"<ul> <li>VM dimensions {4 CPU; 8 GB RAM; 20 GB Disk</li> </ul>"},{"location":"ClusterTaskTracking/#cluster08","title":"Cluster08","text":"<p>Using CreateVMDefinitions.sh process template files to:</p> <ul> <li>Generate terraform vm definitions, provider definition, and talos installation media</li> <li>Generate Cilium manifests and appends them to talos patch file</li> <li>Generate createCluster.sh, which generates (patched) talos configs and apply then to the machines in the cluster</li> <li>createCluster.sh also generates command snippets for talos bootstrap, kube config, flux bootstrap, dashboard access etc</li> </ul> <p>VM dimensions</p> <ul> <li>4 CPU</li> <li>8 GB RAM</li> <li>20 GB Disk</li> </ul>"},{"location":"ClusterTaskTracking/#cluster05","title":"Cluster05","text":"<ul> <li><code>talosctl gen config cluster05 https://192.168.4.114:6443 --config-patch @patch.yaml</code></li> <li><code>talosctl apply-config --insecure --nodes 192.168.4.114 --file controlplane.yaml</code></li> <li><code>talosctl apply-config --insecure --nodes 192.168.4.115 --file worker.yaml</code></li> <li><code>talosctl bootstrap --nodes 192.168.4.114 --endpoints 192.168.4.114 --talosconfig=./talosconfig</code></li> <li><code>talosctl kubeconfig --nodes 192.168.4.114 --endpoints 192.168.4.114 --talosconfig=./talosconfig</code></li> <li><code>kubectl apply -f cilium.yaml</code></li> <li><code>kubectl apply --server-side -k \"https://github.com/piraeusdatastore/piraeus-operator/config/default?ref=v2.8.1\"</code></li> <li>Create Github token, export as GITHUB_TOKEN, with GITHUB_USER</li> <li><code>flux bootstrap github --context=admin@cluster05 --owner=MoTTTT --repository=venus --branch=main --personal --path=clusters/cluster05 --token-auth=true</code></li> </ul> <p>To create flux resource for piraeus:</p> <p>flux create source git piraeus --url=https://github.com/piraeusdatastore/piraeus-operator/config/default --tag=\"v2.8.1\"</p>"},{"location":"ClusterTaskTracking/#cluster03","title":"Cluster03","text":"<p>Using patches, with talos image supporting:</p> <ul> <li>QEMU guest agent</li> <li>DRDB, required for LinStore</li> </ul> <pre><code>customization:\n    systemExtensions:\n        officialExtensions:\n            - siderolabs/drbd\n            - siderolabs/qemu-guest-agent\n</code></pre> <ul> <li><code>talosctl gen config cluster03 https://192.168.4.216:6443 --config-patch @patch.yaml</code></li> <li><code>talosctl apply-config --insecure --nodes 192.168.4.216 --file controlplane.yaml</code></li> </ul> <pre><code>machine:\n  install:\n    disk: /dev/vda\n    image: ?????\n  kernel:\n    modules:\n      - name: drbd\n        parameters:\n          - usermode_helper=disabled\n      - name: drbd_transport_tcp\n  env:\n    https_proxy: http://192.168.4.1:3128/\ncluster:\n  network:\n    cni:\n      name: none\n  proxy:\n    disabled: true\n  allowSchedulingOnControlPlanes: true\n</code></pre>"},{"location":"ClusterTaskTracking/#cluster02","title":"Cluster02","text":"<p>Manually editing controlplane.yaml and worker.yaml files.</p> <ul> <li>Generate cluster config: <code>talosctl gen config cluster02 https://192.168.4.210:6443</code></li> <li>Check the storage device to use: <code>talosctl -n 192.168.4.210 disks --insecure</code></li> <li>Edit controlplane.yaml and worker.yaml files. Set IP address and storage devices.</li> <li><code>talosctl apply-config --insecure --nodes 192.168.4.210 --file controlplane.yaml</code></li> <li><code>talosctl apply-config --insecure --nodes 192.168.4.211 --file worker.yaml</code></li> <li><code>talosctl --talosconfig=./talosconfig --nodes 192.168.4.211 -e 192.168.4.210 version</code></li> <li><code>talosctl bootstrap --nodes 192.168.4.210 --endpoints 192.168.4.210 --talosconfig=./talosconfig</code></li> <li><code>talosctl kubeconfig --nodes 192.168.4.210 --endpoints 192.168.4.210 --talosconfig=./talosconfig</code></li> <li><code>talosctl --nodes 192.168.4.210 --endpoints 192.168.4.210 health --talosconfig=./talosconfig</code></li> <li><code>talosctl --nodes 192.168.4.211 --endpoints 192.168.4.210 dashboard --talosconfig=./talosconfig</code></li> </ul>"},{"location":"ClusterTaskTracking/#generation-0-configuration-on-mars","title":"Generation 0: Configuration on Mars","text":"<ul> <li>ProxMox: Load image into local-lvm: https://factory.talos.dev/image/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515/v1.8.2/nocloud-amd64.iso</li> <li>Proxmox: Create VMs, with 4 CPU, 16 GB RAM, and 100 GB Disk, using talos nocloud-amd64.iso</li> <li>ProxMox: Add CloudInit drive, and set IP addresses {192.168.4.100,192.168.4.101,192.168.4.103}</li> <li>Generate cluster config: <code>talosctl gen config mars01 https://192.168.4.100:6443 --output-dir mars01</code></li> <li>Modify generated <code>controlplane.yaml</code> file, and place in /var/lib/vz/snippets/controlplane.yaml (ensure that local-lvm supports snippets)</li> <li>Control plane: add config to CloudInit drive: <code>qm set $VMID --cicustom user=local:snippets/controlplane.yaml</code></li> <li>Worker: config to CloudInit drive: <code>qm set $VMID --cicustom user=local:snippets/worker.yaml</code></li> <li>Bootstrap cluster: <code>talosctl bootstrap -n 192.168.4.100</code></li> <li>Get kubectl config: <code>talosctl kubeconfig -n 192.168.4.100 --endpoints 192.168.4.100</code></li> <li>Create git repo Mars, token with admin privileges, export as GITHUB_TOKEN, with GITHUB_USER</li> <li>Bootstrap gitops: <code>flux bootstrap github --context=admin@mars01 --owner=MoTTTT --repository=mars --branch=main --personal --path=clusters/mars01 --token-auth=true</code></li> <li>Add supporting infrastructure manifests in Mars repo: MetalLB; ingres-nginx; cert-manager;</li> <li>Add application manifests in Mars repo</li> <li>Get disk info: <code>talosctl -n 192.168.4.100,192.168.4.101,192.168.4.103 disks</code></li> </ul>"},{"location":"ClusterTaskTracking/#controlplaneyaml-deviations","title":"controlplane.yaml deviations","text":"<ul> <li>cilium</li> </ul> <pre><code>cluster:\n  network:\n    cni:\n      name: none\n  proxy:\n    disabled: true\n  allowSchedulingOnControlPlanes: true\n</code></pre> <ul> <li>proxy</li> </ul> <pre><code>machine:\n    env:\n        https_proxy: http://192.168.3.1:3128/\n</code></pre> <p>talosctl gen config talos-nocloud https://192.168.30:6443 --config-patch @patch.yaml</p>"},{"location":"ClusterTaskTracking/#lab-environments","title":"Lab environments","text":"<ul> <li>Server: Mars with 32 CPU; 220 GB RAM; 2 X 1TB, 4 X 600GB Disk</li> <li>Server: Mercury with 32 CPU; 220 GB RAM; 2 X 1TB, 4 X 600GB Disk</li> <li>Server: Venus: Proxmox on R730 with 64 CPUs; 160GB RAM; 1.6 TB zfs pool</li> </ul>"},{"location":"ClusterTaskTracking/#talosctl-client-configuration","title":"Talosctl client configuration","text":"<ul> <li>Mac: <code>brew install siderolabs/tap/talosctl</code></li> <li>On unix, need to first install brew dependencies: <code>sudo apt-get install build-essential procps curl file git</code></li> <li>Then need to install brew: <code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"</code></li> <li>Add brew to path: <code>alias brew=\"/home/linuxbrew/.linuxbrew/Homebrew/bin/brew\"</code></li> <li>Then talosctl: <code>brew install siderolabs/tap/talosctl</code></li> <li><code>alias talosctl=\"/home/linuxbrew/.linuxbrew/Homebrew/Cellar/talosctl/1.9.1/bin/talosctl\"</code></li> <li>Then you can use (or link to): <code>/home/linuxbrew/.linuxbrew/Cellar/talosctl/1.9.1/bin/talosctl</code></li> <li><code>alias talosctl='/home/linuxbrew/.linuxbrew/Homebrew/Cellar/talosctl/1.9.1/bin/talosctl --talosconfig=/home/colleymj/.talos/talosconfig'</code></li> </ul>"},{"location":"ClusterTaskTracking/#networking","title":"Networking","text":"<ul> <li>Flannel installs by default</li> <li>For enterprise CNI features, installing Cilium</li> <li>PXE Boot:</li> </ul> <p>Use PXE for boot, with metadata service for per machine (MAC Address) configuration.</p> <pre><code>talos.config=https://metadata.service/talos/config?mac=${mac}\n</code></pre>"},{"location":"ClusterTaskTracking/#talos-internet-proxy-support","title":"Talos internet proxy support","text":"<p>For initial boot, add command line arguments: <code>talos.environment=http_proxy=http://192.168.4.51:3128 talos.environment=https_proxy=http://192.168.4.51:3128</code></p> <p>Talos image factory specification, supporting drbd fir linode (existing), qemu-guest-agent for terraform remote control (existing), and a new squid proxy, on the cluster hardware.</p> <pre><code>customization:\n    extraKernelArgs:\n        - talos.environment=http_proxy=http://192.168.4.51:3128\n        - talos.environment=https_proxy=http://192.168.4.51:3128\n    systemExtensions:\n        officialExtensions:\n            - siderolabs/drbd\n            - siderolabs/qemu-guest-agent\n</code></pre> <p>For runtime, add machine configuration to the patch:</p> <pre><code>machine:\n  env:\n    http_proxy: http://192.168.4.51:3128\n    https_proxy: http://192.168.4.51:3128\n    no_proxy: \"localhost,127.0.0.1,192.168.4/24,10.244.0.0/16,10.96.0.0/12\"\n</code></pre>"},{"location":"ClusterTaskTracking/#serving-manifests-and-images-locally","title":"Serving manifests and images locally","text":"<ul> <li>Apache set up on a VM: http://192.168.4.5/</li> </ul> <p>Supported manifests:</p> <ul> <li>kubelet-serving-cert-approver.yaml</li> <li>metrics-server.yaml</li> <li>piraeus-operator.yaml</li> <li>gateway-api.yaml</li> <li>cilium.yaml</li> </ul> <p>Supported images:</p> <ul> <li>https://factory.talos.dev/image/TALOSIMAGEID/${local.talos.version}/nocloud-amd64.iso</li> <li>TALOSIMAGEID: <code>ed7716909fb764e0c322ab43dd20918e30cf8ffa3914ba3fa229afec9efe4d84</code></li> <li>Talos version: <code>v1.10.2</code></li> <li>http://192.168.4.5/nocloud-amd64.iso</li> </ul>"},{"location":"ClusterTaskTracking/#cluster-components","title":"Cluster components","text":"<ul> <li>https://github.com/adampetrovic/home-ops/tree/main</li> <li>https://www.cloudraft.io/blog/making-kubernetes-simple-with-talos</li> <li>https://blog.devops.dev/talos-os-raspberry-fc5f327b7026</li> <li>https://kevinholditch.co.uk/2023/10/21/creating-a-kubernetes-cluster-using-talos-linux-on-xen-orchestra/</li> </ul>"},{"location":"ClusterTaskTracking/#references","title":"References","text":"<ul> <li>https://www.talos.dev/v1.8/introduction/prodnotes/</li> <li>https://factory.talos.dev/?arch=amd64&amp;cmdline-set=true&amp;extensions=-&amp;extensions=siderolabs%2Fiscsi-tools&amp;extensions=siderolabs%2Fqemu-guest-agent&amp;platform=nocloud&amp;target=cloud&amp;version=1.8.2</li> <li>https://www.talos.dev/v1.8/talos-guides/install/virtualized-platforms/proxmox/</li> <li>https://www.talos.dev/v1.8/talos-guides/install/cloud-platforms/nocloud/</li> <li>https://blog.devops.dev/talos-os-raspberry-fc5f327b7026</li> <li>https://www.civo.com/blog/calico-vs-flannel-vs-cilium</li> <li>https://cilium.io/</li> <li>https://www.cloudraft.io/blog/making-kubernetes-simple-with-talos</li> <li>https://github.com/adampetrovic/home-ops/blob/main/kubernetes/bootstrap/talos/talconfig.yaml</li> <li>KaaS GitOps: https://github.com/kubebn/talos-proxmox-kaas</li> <li>PXE: https://www.talos.dev/v1.8/talos-guides/install/bare-metal-platforms/pxe/</li> <li>Options: https://www.civo.com/blog/calico-vs-flannel-vs-cilium</li> <li>https://github.com/cilium/cilium</li> <li>https://github.com/cilium/cilium/blob/v1.18.1/install/kubernetes/cilium/values.yaml#L992</li> </ul>"},{"location":"Flux/","title":"Flux","text":""},{"location":"Flux/#introduction","title":"Introduction","text":"<p>Flux is a Kubernetes GitOps Operator. GitOps is the practice of using code repositories as the single source of truth for cluster and workload configuration.</p> <p>Flux was built by Weaveworks, and donated to the Cloud Native Computing Foundation (CNCF).</p> <p>Instrumenting a cluster with Flux requires the following:</p> <ul> <li>A git repo for the cluster definition, with its associated access username and token exported.</li> <li>Installing the flux utility on a cluster administrator workstation (with kubectl context set for the target cluster).</li> <li>Bootstrapping flux, specifying the git account, repo, branch and path.</li> </ul> <p>When the bootstrap process concludes, the GIT repo will contain the flux-system component manifests, and the target cluster will have flux-system namespace operator components installed. The flux operator with thereafter monitor the git repo for cluster configuration requirement changes, and apply the appropriate cluster adjustments.</p>"},{"location":"Flux/#dev-environment-bootstrap","title":"dev environment bootstrap","text":"<pre><code>flux bootstrap github --context=microk8s --owner=MoTTTT --repository=podzonedev-gitops --branch=main --personal --path=clusters/megalith --token-auth=true\n</code></pre>"},{"location":"Flux/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>https://fluxcd.io/flux/cheatsheets/troubleshooting/</li> </ul>"},{"location":"Flux/#notes","title":"Notes","text":"<ul> <li>Mac OS flux Installation: <code>brew install fluxcd/tap/flux</code></li> <li>Unix install <code>curl -s https://fluxcd.io/install.sh | sudo bash</code></li> <li>Create a GITHUB token, and export username and token: <code>export GITHUB_TOKEN=&lt;token&gt;</code>; <code>export GITHUB_USER=MoTTTT</code></li> <li>Check flux and cluster: <code>flux check --pre</code></li> <li>Bootstrap: <code>flux bootstrap github --context=prod --owner=MoTTTT --repository=admin --branch=main --personal --path=clusters/prod --token-auth=true</code></li> <li>GitHub token expiry: Delete flux secret, and bootstrap again with new token in context.</li> </ul>"},{"location":"Flux/#resources-created-by-flux","title":"Resources created by flux","text":"<p>Can be seen on delete: <code>flux uninstall</code></p> <pre><code>martincolley@Dolmen:~/workspace/admin/clusters/ukdev$ flux uninstall\nAre you sure you want to delete Flux and its custom resource definitions: y\n\u25ba deleting components in flux-system namespace\n\u2714 Deployment/flux-system/helm-controller deleted \n\u2714 Deployment/flux-system/kustomize-controller deleted \n\u2714 Deployment/flux-system/notification-controller deleted \n\u2714 Deployment/flux-system/source-controller deleted \n\u2714 Service/flux-system/notification-controller deleted \n\u2714 Service/flux-system/source-controller deleted \n\u2714 Service/flux-system/webhook-receiver deleted \n\u2714 NetworkPolicy/flux-system/allow-egress deleted \n\u2714 NetworkPolicy/flux-system/allow-scraping deleted \n\u2714 NetworkPolicy/flux-system/allow-webhooks deleted \n\u2714 ServiceAccount/flux-system/helm-controller deleted \n\u2714 ServiceAccount/flux-system/kustomize-controller deleted \n\u2714 ServiceAccount/flux-system/notification-controller deleted \n\u2714 ServiceAccount/flux-system/source-controller deleted \n\u2714 ClusterRole/crd-controller-flux-system deleted \n\u2714 ClusterRole/flux-edit-flux-system deleted \n\u2714 ClusterRole/flux-view-flux-system deleted \n\u2714 ClusterRoleBinding/cluster-reconciler-flux-system deleted \n\u2714 ClusterRoleBinding/crd-controller-flux-system deleted \n\u25ba deleting toolkit.fluxcd.io finalizers in all namespaces\n\u2714 GitRepository/flux-system/flux-system finalizers deleted \n\u2714 Kustomization/flux-system/flux-system finalizers deleted \n\u25ba deleting toolkit.fluxcd.io custom resource definitions\n\u2714 CustomResourceDefinition/alerts.notification.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/buckets.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/gitrepositories.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/helmcharts.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/helmreleases.helm.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/helmrepositories.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/kustomizations.kustomize.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/ocirepositories.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/providers.notification.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/receivers.notification.toolkit.fluxcd.io deleted \n\u2714 Namespace/flux-system deleted \n\u2714 uninstall finished\n</code></pre>"},{"location":"Network/","title":"Network Architecture","text":""},{"location":"Network/#proxmox-hypervisor-running-on-a-poweredge-r730","title":"Proxmox Hypervisor running on a PowerEdge R730","text":"<ul> <li>The R730 runs on an isolated network, with the bastion server, having two network interfaces, forming a glasshouse function.</li> <li>Web traffic from the internet to any supported domains are port forwarded to the bastion server</li> <li>The bastion server provides internet gateway services to the isolated network.</li> <li>Various services in the isolated network are made available via the bastion server for local access.</li> </ul> <pre><code>---\ntitle: venus.podzone.net\n---\ngraph TD\nInternet  -- :80 :443 --&gt; router -- :80 :443 --&gt; bastion\nworkstation --&gt; bastion\nbastion -- 192.168.4.50 --&gt; proxmoxAPI\nbastion -- 192.168.4.198 --&gt; ingress \nbastion -- 192.168.4.199 --&gt; g1[gateway]\nbastion -- 192.168.4.209 --&gt; g2[gateway]\nbastion -- 192.168.4.200 --&gt; c2[k8s API]\nbastion -- 192.168.4.247 --&gt; iDRAC\nbastion -- 192.168.4.190 --&gt; c1[k8s API]\nsubgraph venus\n    subgraph kubernetes cluster09\n      ingress --&gt; service1\n      ingress --&gt; serviceN\n      g1 --&gt; httpsroute1\n      g1 --&gt; httpsrouteN\n      c1\n    end\n    subgraph Cluster Management cluster\n      c2 --&gt;  CAPI\n      g2 --&gt; manifestServer\n      g2 --&gt; imageCache\n      c2\n    end\n  proxmoxAPI\n  iDRAC\n  Disk((4TB DISK))\nend \n</code></pre>"},{"location":"Network/#provisioning","title":"Provisioning","text":"<ul> <li>https://github.com/christensenjairus/ClusterCreator</li> <li>https://cyber-engine.com/blog/2024/06/25/k8s-on-proxmox-using-clustercreator/</li> <li>https://github.com/DushanthaS/kubernetes-the-hard-way-on-proxmox</li> <li>https://github.com/siderolabs/image-factory</li> <li>https://blog.stonegarden.dev/articles/2024/08/talos-proxmox-tofu/</li> <li>https://github.com/rgl/terraform-proxmox-talos</li> <li>https://olav.ninja/talos-cluster-on-proxmox-with-terraform</li> <li>https://github.com/kubernetes-sigs/kubespray</li> <li>https://kubespray.io/</li> <li>https://blog.andreasm.io/2024/01/15/proxmox-with-opentofu-kubespray-and-kubernetes/</li> <li>https://medium.com/@abhigyan.dwivedi_58961/creating-a-kvm-kubernetes-cluster-with-vagrant-kubespray-and-ansible-a-idiot-resistant-guide-2f3727ce7039</li> </ul>"},{"location":"Network/#monitoring","title":"Monitoring","text":"<ul> <li>https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack</li> <li>https://github.com/prometheus-operator/kube-prometheus</li> <li>Network Monitoring: https://docs.cilium.io/en/stable/overview/intro/</li> <li>Kubernetes Document Generation: https://github.com/philippemerle/KubeDiagrams</li> <li>Document Generation: https://www.graphviz.org/</li> </ul>"},{"location":"Network/#monitoring-hardware","title":"Monitoring hardware","text":"<ul> <li>IPMI Tool suite: https://www.gnu.org/software/freeipmi/</li> <li>Dell idrac exporter: https://github.com/galexrt/dellhw_exporter</li> <li>Dell Redfish idrac exporter: https://github.com/mrlhansen/idrac_exporter</li> <li>Redfish idrac exporter: https://github.com/jenningsloy318/redfish_exporter</li> <li>Prometheus ipmi exporter: https://github.com/prometheus-community/ipmi_exporter</li> <li>SNMP Exporter: https://github.com/prometheus/snmp_exporter</li> <li>Helm chart for SNMP Exporter: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-snmp-exporter</li> <li>Node-Exporter ipmitool collector: https://github.com/prometheus-community/node-exporter-textfile-collector-scripts/blob/master/ipmitool</li> <li>Grafana Dashboard for idrac: https://grafana.com/grafana/dashboards/13177-ipmi-for-prometheus/</li> </ul>"},{"location":"Network/#networking","title":"Networking","text":"<ul> <li>Creating template files: https://github.com/trfore/proxmox-template-scripts?tab=readme-ov-file</li> <li>For Bastion: IP Tables Load Balancer: https://github.com/muzahid-c/iptables-loadbalancer</li> </ul>"},{"location":"Network/#cloud-init","title":"Cloud init","text":"<ul> <li>https://austinsnerdythings.com/2021/08/30/how-to-create-a-proxmox-ubuntu-cloud-init-image/</li> </ul>"},{"location":"Network/#storage","title":"Storage","text":"<ul> <li>https://github.com/LINBIT/linstor-server</li> <li>https://linbit.com/drbd/</li> <li>https://github.com/piraeusdatastore/piraeus-operator</li> <li>https://syncthing.net/</li> <li>https://github.com/sergelogvinov/proxmox-csi-plugin</li> </ul>"},{"location":"Storage/","title":"Storage","text":""},{"location":"Storage/#zfs-storage-configuration-on-hypervisor","title":"ZFS storage configuration on hypervisor","text":"<ul> <li>Hypervisor storage resources: 8 X 600GB SAS Disk, configured in IT mode</li> <li>1 Disk assigned to hypervisor (no redundancy, SPOF)</li> <li>7 disks configured in raid 0 array (Any of 7 disks SPOF)</li> <li>Optimising for disk utilisation, and speed</li> <li>Optimising for cluster build repeatability</li> </ul>"},{"location":"Storage/#example-raid-0-configuration","title":"Example Raid 0 configuration","text":"<ul> <li>List available volumes: <code>ls -la /dev/disk/by-id</code></li> <li>Create pool: <code>zpool create pool1 wwn-0x5000cca05633e2d0 ... wwn-0x500003930821ed54</code></li> <li>Add pool: <code>pvesm add zfspool pool1 -pool pool1</code></li> </ul>"},{"location":"Storage/#distributed-storage","title":"Distributed storage","text":"<ul> <li>Using in-cluster Linstor</li> </ul>"},{"location":"Storage/#linstor-installation","title":"Linstor installation","text":"<ul> <li>Check talos extensions: <code>talosctl --talosconfig=./talosconfig --endpoints 192.168.4.114 --nodes 192.168.4.114 get extensions</code></li> <li>Check talos loaded modules: <code>talosctl --talosconfig=./talosconfig --endpoints 192.168.4.114 --nodes 192.168.4.114 read /proc/modules</code></li> <li>Check talos drbd config: <code>talosctl --talosconfig=./talosconfig --endpoints 192.168.4.114 --nodes 192.168.4.114 read /sys/module/drbd/parameters/usermode_helper</code></li> <li><code>kubectl apply --server-side -k \"https://github.com/piraeusdatastore/piraeus-operator/config/default?ref=v2.8.1\"</code></li> </ul> <p>Apply overrides:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: talos-loader-override\nspec:\n  podTemplate:\n    spec:\n      initContainers:\n        - name: drbd-shutdown-guard\n          $patch: delete\n        - name: drbd-module-loader\n          $patch: delete\n      volumes:\n        - name: run-systemd-system\n          $patch: delete\n        - name: run-drbd-shutdown-guard\n          $patch: delete\n        - name: systemd-bus-socket\n          $patch: delete\n        - name: lib-modules\n          $patch: delete\n        - name: usr-src\n          $patch: delete\n        - name: etc-lvm-backup\n          hostPath:\n            path: /var/etc/lvm/backup\n            type: DirectoryOrCreate\n        - name: etc-lvm-archive\n          hostPath:\n            path: /var/etc/lvm/archive\n            type: DirectoryOrCreate\n</code></pre> <p>Create Linstor cluster:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec: {}\n</code></pre> <p>Check node list: <code>kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor node list</code></p> <p>Configure storage:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: storage-pool\nspec:\n  storagePools:\n    - name: pool1\n      fileThinPool:\n        directory: /var/lib/piraeus-datastore/pool1\n</code></pre> <p>Check storage pool: <code>kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor storage-pool list</code></p> <p>Create storage class:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: piraeus-storage\nprovisioner: linstor.csi.linbit.com\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  linstor.csi.linbit.com/storagePool: pool1\n</code></pre> <p>Replicated storage class:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: piraeus-storage-replicated\nprovisioner: linstor.csi.linbit.com\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  linstor.csi.linbit.com/storagePool: pool1\n  linstor.csi.linbit.com/placementCount: \"2\"\n</code></pre> <p>Snapshots: https://piraeus.io/docs/stable/tutorial/snapshots</p> <p>Deploy snapshot controller</p> <pre><code>kubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//client/config/crd\nkubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//deploy/kubernetes/snapshot-controller\n</code></pre> <p>Create SnapshotClass</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: piraeus-snapshots\ndriver: linstor.csi.linbit.com\ndeletionPolicy: Delete\n</code></pre> <p>Create VolumeSnapshot</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: data-volume-snapshot-1\nspec:\n  volumeSnapshotClassName: piraeus-snapshots\n  source:\n    persistentVolumeClaimName: data-volume\n</code></pre> <p>Affinity controller:</p> <p><code>helm repo add piraeus-charts https://piraeus.io/helm-charts/</code> <code>helm install linstor-affinity-controller piraeus-charts/linstor-affinity-controller</code></p> <p>Monitoring with Prometheus operator:</p> <p>Install prometheus:</p> <p><code>$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts</code></p> <pre><code>helm install --create-namespace -n monitoring prometheus prometheus-community/kube-prometheus-stack \\\n  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\\n  --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \\\n  --set prometheus.prometheusSpec.ruleSelectorNilUsesHelmValues=false\n</code></pre> <p>Add monitoring and alert rules for piraeus:</p> <pre><code>kubectl apply --server-side -n piraeus-datastore -k \"https://github.com/piraeusdatastore/piraeus-operator//config/extras/monitoring?ref=v2\"\n</code></pre>"},{"location":"Storage/#switching-to-gitops","title":"Switching to gitops","text":"<ul> <li><code>flux create source helm piraeus-charts  --url=https://prometheus-community.github.io/helm-charts --interval=10m --export</code></li> <li><code>flux create hr affinity-controller  --interval=10m  --source=HelmRepository/piraeus-charts  --chart=linstor-affinity-controller --export</code></li> <li>See gitops-infra for manifests.</li> </ul>"},{"location":"Storage/#references","title":"References","text":"<ul> <li>Reference: https://piraeus.io/docs/stable/tutorial/get-started</li> </ul>"}]}