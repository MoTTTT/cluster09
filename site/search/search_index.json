{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cluster09, a PodZone project","text":"<p>This repo contains a bootstraping and scaffolding and configuration for</p> <ul> <li>A Cluster API management cluster</li> <li>Cluster API template for a kubernetes cluster</li> <li>Template input to instantiate a managed cluster (Cluster09)</li> <li>Cluster09 gitops infrastructure configuration (Storage, Networking, Gateway)</li> <li>Cluster09 gitops workload configuration (Visibility tools)</li> <li>Cluster09 gitops authC authZ services (Fine grained and delegated access control</li> </ul>"},{"location":"#cluster-api-management-cluster","title":"Cluster API management cluster","text":"<ul> <li>Scripts to substitute instance variables into templates, for the bootstrapping of a Cluster API management cluster, and providing notes on user driven configuration.</li> <li>Templates generated Teraform VM definitions</li> <li>Templates for generated Talos configuration</li> <li>Script for Talos configuration form generated configurations</li> </ul>"},{"location":"#technology-considerations","title":"Technology considerations","text":""},{"location":"#specification","title":"Specification","text":"<ul> <li>Automated, repeatable cluster provisioning</li> <li>Provisioning scaffolding, (clean slate environments)</li> <li>Isolated provisioning (airgapped environment bootstrap)</li> </ul>"},{"location":"#dependency-management","title":"Dependency management","text":"<p>To support airgapped green-fields deployment, all dependencies need to be met locally. To leverage automated deployments, the following cached (and therefore version controlled) information is required.</p> <ul> <li>Local image mirror</li> <li>local config repo, with documentation, configuration, and scripts</li> <li>manifest repo, with cluster09 cluster and workload manifests</li> <li>tested with network airgapped bootstrap to workload commissioned</li> <li>Type approved bastion or bootstrap server configuration and build notes</li> </ul>"},{"location":"#capi-process-component-and-workflow","title":"CAPI: Process, Component, and Workflow","text":"<ul> <li>Talos for CAPI management cluster</li> <li>Terraform for CAPI cluster VMs</li> <li>Bash for CAPI Terraform and Talos template substitution</li> <li>Manual Terraform execution</li> <li>Scripted Talos Installation for CAPI</li> <li>Manual Talos cluster bootstrapping</li> <li>Manual access configuration extraction</li> <li>Manual gitops CAPI cluster workload bootstrapping (for future cluster API operator)</li> <li>Manual clusterctl CAPI cluster workload bootstrapping </li> </ul>"},{"location":"#cluster09-process-components-and-workflow","title":"Cluster09: Process, Components, and Workflow","text":"<ul> <li>Cluster template for proxmox and talos provided cluster</li> <li>Manual cluster configuration</li> <li>clustercli manifest generation from template, and configuration</li> <li>gitops  </li> </ul>"},{"location":"#platform-process-components-and-workflow","title":"Platform: Process, Components, and Workflow","text":"<ul> <li>Proxmox hypervisor</li> <li>Talos for management and provisioned clusters (Cluster09)</li> </ul>"},{"location":"#proxmox-hypervisor","title":"Proxmox hypervisor","text":"<ul> <li>Proxmox V9 installed</li> </ul>"},{"location":"Bastion/","title":"Bastion Server","text":"<p>The bastion server is a server configured on the network to have visibility from both inside and outside of the machine network, and serves as the only route into or out of the cluster. A linux server (virtual or physical) with two network interfaces achieves this, together with the required software installations and configuration. The configuration described can serve as a bastion server for multiple clusters. In a scaled up architecture, redundant distributed operator tooling and network ingress/egress functions should be used.</p> <pre><code>---\ntitle: Bastion Server\n---\nflowchart LR\n\nw1[workstation] --&gt;\n\nn1[LAN] --&gt; n2[Bastion Server]\nn2 -- PortForward&lt;br&gt;Reverse Proxy --&gt; n3[machine network]\nn3 -- Gateway --&gt; n2\n\nnfs &lt;--&gt; n3\nharbor &lt;--&gt; n3\nn3 --Internet Proxy--&gt; squid\nsquid -- Gateway --&gt; n2\n\nclassDef box fill:#fff,stroke:#000,stroke-width:1px,color:#000;\nclassDef spacewhite fill:#ffffff,stroke:#fff,stroke-width:0px,color:#000\nclass sc1,sc2,sc3,w1,w2,w3,w4 box\n</code></pre> <p>The following software installations may be required to run day 0 operations from the command line or from a remote pipeline:</p> <ul> <li>kubectl</li> <li>talosctl</li> <li>terraform</li> <li>flux</li> </ul> <p>The following server software needs to be installed on the bastion server:</p> <ul> <li>apache: For serving occasional artefacts to the clusters, and for ingress/gateway reverse-proxy and load balancing</li> </ul> <p>Ingress:</p> <ul> <li>apache listens on all networks on port 80 and 443</li> <li>hostname based reverse proxy rules route appropriate traffic to the cluster's or clusters' http/s listeners</li> <li>to separate the control network from direct Internet ingress and egress, control network traffic is managed with iptables port forwarding</li> </ul> <p>Egress:</p> <ul> <li>NAT: The bastion server is set up as gateway for the cluster networks, with iptables NAT configured for direct egress</li> <li>Internet proxy: Caching of artefacts from the Internet provides efficiencies and robustness. The cluster infrastructure and southern caches are configured to leverage this, adding a security visibility point.</li> </ul> <p>Supporting infrastructure:</p> <ul> <li>nfs</li> <li>harbor</li> <li>squid</li> </ul> <p>The following activities are required to build a cluster and bootstrap to a gitops repo:</p> <ul> <li>Git pull</li> <li>Configuration Management</li> <li>Cluster virtual machine build</li> <li>Cluster node build</li> <li>Cluster bootstrap</li> <li>Configuration retrieval</li> <li>GitOps bootstrap</li> <li>Cluster Infrastructure Deployment</li> <li>Workload deployment</li> </ul>"},{"location":"BastionDiagram/","title":"BastionDiagram","text":""},{"location":"BastionDiagram/#network","title":"Network","text":"<pre><code>---\ntitle: Network\n---\nflowchart LR\n\nn0[Internet] -- StaticIP --&gt; n1[ISP Router] -- PortForward --&gt; n2[Bastion Router]\nn2 -- PortForward&lt;br&gt;Reverse Proxy --&gt; n3[machine network]\nn5[Operator client] -- ssh --&gt; n2\n\nn3 --&gt; sc1[control plane node 1]\nn3 --&gt; sc2[control plane node 2]\nn3 --&gt; sc3[control plane node 3]\nn3 --&gt; w1[worker node 1]\nn3 --&gt; w2[worker node 2]\nn3 --&gt; w3[worker node 3]\nn3 --&gt; w4[worker node 4]\n\nn3 --Internet Proxy--&gt; squid -- Gateway --&gt; n2\nn3 -- Image mirror --&gt; harbor -- Gateway --&gt; n2\nn3 -- Gateway --&gt; n2\nn3 -- File store --&gt; nfs &lt;-- Port forwarding --&gt; n2 \n\nn4[cluster network]\n\nsc1 &lt;-.-&gt; n4\nsc2 &lt;-.-&gt; n4\nsc3 &lt;-.-&gt; n4\nw1 &lt;-.-&gt; n4\nw2 &lt;-.-&gt; n4\nw3 &lt;-.-&gt; n4\nw4 &lt;-.-&gt; n4\n\nclassDef box fill:#fff,stroke:#000,stroke-width:1px,color:#000;\nclassDef spacewhite fill:#ffffff,stroke:#fff,stroke-width:0px,color:#000\nclass sc1,sc2,sc3,w1,w2,w3,w4,squid,harbor,nfs box\n</code></pre>"},{"location":"Cilium/","title":"Cilium","text":""},{"location":"Cilium/#diagnostics","title":"Diagnostics","text":"<ul> <li>Monitor: <code>kubectl -n kube-system exec ds/cilium -- cilium-dbg monitor</code></li> <li>Service list: <code>kubectl -n kube-system exec ds/cilium -- cilium-dbg service list</code></li> <li>Verbose status: <code>kubectl -n kube-system exec ds/cilium -- cilium-dbg status --verbose</code></li> </ul>"},{"location":"Cilium/#generating-the-cilium-manifest","title":"Generating the cilium manifest","text":"<p>Missing in Management Cluster Iteration 3 for Management Cluster:</p> <pre><code>\n l7Proxy=true\n envoyConfig.enabled=true\n</code></pre> <p>Additional settings from https://docs.siderolabs.com/kubernetes-guides/cni/deploying-cilium</p> <pre><code>---\n\n    --set gatewayAPI.enableAlpn=true \\\n    --set gatewayAPI.enableAppProtocol=true\n</code></pre> <ul> <li><code>helm repo add cilium https://helm.cilium.io/</code></li> <li><code>helm repo update</code></li> </ul> <pre><code>helm template \\\n    cilium \\\n    cilium/cilium \\\n    --version 1.17.4 \\\n    --set hl7Proxy.enabled=true \\\n    --set envoyConfig.enabled=true \\\n    --set hubble.relay.enabled=true \\\n    --set hubble.ui.enabled=true \\\n    --set ingressController.enabled=true \\\n    --set ingressController.loadbalancerMode=shared \\\n    --set ingressController.default=true \\\n    --set l2announcements.enabled=true \\\n    --set l2announcements.leaseDuration=3s \\\n    --set l2announcements.leaseRenewDeadline=1s \\\n    --set l2announcements.leaseRetryPeriod=200ms \\\n    --set loadBalancerIPs.enable=true \\\n    --set gatewayAPI.enabled=true \\\n    --set gatewayAPI.enableAlpn=true \\\n    --set gatewayAPI.enableAppProtocol=true \\\n    --set loadBalancer.l7.backend=envoy \\\n    --namespace kube-system \\\n    --set ipam.mode=kubernetes \\\n    --set kubeProxyReplacement=true \\\n    --set securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n    --set securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n    --set cgroup.autoMount.enabled=false \\\n    --set cgroup.hostRoot=/sys/fs/cgroup \\\n    --set k8sServiceHost=localhost \\\n    --set k8sServicePort=7445 &gt; cilium.yaml\n</code></pre> <p>This enables:</p> <ul> <li>hubble, and hubble ui</li> <li>ingres controller, set to shared loadbalancer</li> <li>l2 announcements on loadbalancer IPs</li> <li>gateway api</li> </ul> <p>Add IP pool:</p> <pre><code>apiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumLoadBalancerIPPool\nmetadata:\n  name: \"ip-pool\"\nspec:\n  blocks:\n  - start: \"192.168.4.90\"\n    stop: \"192.168.4.90\"\n</code></pre> <p>Add L2 announcement policy:</p> <pre><code>apiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumL2AnnouncementPolicy\nmetadata:\n  name: l2policy\nspec:\n  loadBalancerIPs: true\n</code></pre>"},{"location":"Cilium/#gateway","title":"Gateway","text":"<p>Example GatewayClass and Gateway, with TLS termination using cert-manager:</p> <pre><code>---\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: GatewayClass\nmetadata:\n  name: cilium\nspec:\n  controllerName: io.cilium/gateway-controller\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: cluster08\n  namespace: kube-system\n  annotations:\n    cert-manager.io/cluster-issuer: lets-encrypt\nspec:\n  gatewayClassName: cilium\n  listeners:\n  - hostname: cluster08.podzone.cloud\n    name: cluster08-podzone-cloud-http\n    port: 80\n    protocol: HTTP\n  - hostname: cluster08.podzone.cloud\n    name: cluster08-podzone-cloud-https\n    port: 443\n    protocol: HTTPS\n    tls:\n      mode: Terminate\n      certificateRefs:\n        - name: cluster08-secret\n    allowedRoutes:\n      namespaces:\n        from: All\n</code></pre>"},{"location":"Cilium/#implementation-notes","title":"Implementation notes","text":""},{"location":"Cilium/#v1-iteration","title":"V1 Iteration","text":"<ul> <li>Install on talos</li> <li>Generate cilium manifest:</li> <li>For L2 announcements, and cilium ingress controller, set:</li> <li>externalIPs.enabled=true</li> <li>l2announcements.enabled=true</li> <li>ingressController.enabled=true</li> <li>ingressController.loadbalancerMode=shared</li> <li>kubeProxyReplacement=true</li> <li>l7Proxy=true</li> <li>envoyConfig.enabled=true</li> <li>loadBalancer.l7.backend=envoy</li> <li>ingressController.default=true</li> </ul>"},{"location":"Cilium/#v2-installation","title":"V2 installation","text":"<ul> <li>Include GatewayAPI</li> <li>Enable Hubble relay and UI</li> <li>L2 announcement lease config</li> <li>Enable loadBalancerIPs</li> <li>Add to talos controlplane-patch.yaml</li> <li>Create patch template with {proxy,disable cni, disable proxy, asocp, stub for cilium manifest}</li> <li>controlplane-patch-template.yaml</li> </ul>"},{"location":"Cilium/#v3-installation","title":"V3 installation","text":"<ul> <li>Generate cilium manifest, as above, and serve from URL, add to extraManifests in talos config.</li> </ul>"},{"location":"Cilium/#issues","title":"Issues","text":"<ul> <li>Installation of opensearch: <code>invalid: metadata.labels: Invalid value: \\\"opensearch-opensearch-dashboard-opensearch-dashboards-dashboards\\\": must be no more than 63 characters\"</code></li> </ul>"},{"location":"Cilium/#references","title":"References","text":"<ul> <li>https://docs.cilium.io/en/latest/network/servicemesh/tls-termination/</li> <li>https://cilium.io/use-cases/gateway-api/</li> <li>https://docs.cilium.io/en/stable/network/lb-ipam/</li> <li>https://isovalent.com/blog/post/migrating-from-metallb-to-cilium/</li> <li>https://docs.cilium.io/en/stable/network/servicemesh/ingress/</li> <li>https://gateway-api.sigs.k8s.io/</li> <li>https://docs.cilium.io/en/v1.14/network/servicemesh/gateway-api/gateway-api/</li> <li>https://itnext.io/cilium-gateway-api-cert-manager-and-lets-encrypt-updates-cc730818cb17</li> <li>https://blog.grosdouli.dev/blog/cilium-gateway-api-cert-manager-let's-encrypt</li> <li>https://github.com/cilium/cilium/blob/v1.18.1/install/kubernetes/cilium/values.yaml#L992</li> <li>https://cert-manager.io/docs/usage/gateway/</li> </ul>"},{"location":"ClusterAPI/","title":"Cluster API","text":""},{"location":"ClusterAPI/#clusterctl","title":"clusterctl","text":"<ul> <li>Tested successfully with 2 node talos cluster (Aug 2025)</li> <li>Install cert-manager: <code>helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true</code></li> <li>Set up clusterctl config file: <code>~/.cluster-api/clusterctl.yaml</code> (On initial install only CAPI mandatory fields are set)</li> <li>Install components: <code>clusterctl init --ipam in-cluster --core cluster-api -c talos -b talos -i proxmox</code></li> <li>Version installed, based on init response:</li> </ul> <pre><code>Installing provider=\"cluster-api\" version=\"v1.10.5\" targetNamespace=\"capi-system\"\nInstalling provider=\"bootstrap-talos\" version=\"v0.6.9\" targetNamespace=\"cabpt-system\"\nInstalling provider=\"control-plane-talos\" version=\"v0.5.10\" targetNamespace=\"cacppt-system\"\nInstalling provider=\"infrastructure-proxmox\" version=\"v0.7.3\" targetNamespace=\"capmox-system\"\nInstalling provider=\"ipam-in-cluster\" version=\"v1.0.3\" targetNamespace=\"capi-ipam-in-cluster-system\"\n</code></pre> <p>clusterctl generate yaml --list-variables --from cluster-template.yaml &gt; cluster08-variables.yaml clusterctl generate yaml --from cluster-template.yaml &gt; cluster08.yaml </p> <ul> <li>Spin up a cluster: <code>kubectl apply -f proxmoxcluster.yaml</code></li> <li>Describe cluster: <code>clusterctl describe cluster cluster07 -n cluster07 --show-conditions all --show-templates --show-resourcesets --grouping=false --echo</code></li> <li>Retrieve talos config: <code>kubectl get secret --namespace cluster07 cluster07-talosconfig -o jsonpath='{.data.talosconfig}' | base64 -d &gt; cluster07-talosconfig</code></li> <li>Retrieve kubeconfig: <code>talosctl kubeconfig --nodes 192.168.4.170 --endpoints 192.168.4.170 --talosconfig=./cluster07-talosconfig</code></li> <li>To retrieve file: <code>kubectl get secret --namespace cluster07 cluster07-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; cluster07-kubeconfig</code></li> <li>scale up cluster control plane: Change replicas to 3</li> <li>scale up workers: Change replicas to 2</li> <li>bootstrap flux: <code>flux bootstrap github --context=admin@cluster07 --owner=MoTTTT --repository=venus --branch=main --personal --path=clusters/managedcluster07 --token-auth=true</code></li> </ul>"},{"location":"ClusterAPI/#cluster-api-operator","title":"Cluster API Operator","text":"<ul> <li>Install cert-manager: <code>helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true</code></li> <li>Install CAPI Operator: <code>helm install capi-operator capi-operator/cluster-api-operator --create-namespace -n capi-operator-system --set infrastructure.proxmox.enabled=true --set controlPlane.talos.enabled=true --set  bootstrap.talos.enabled=true --wait --timeout 90s</code></li> <li>Configure proxmox credentials: <code>kubectl apply -f capmox-manager-credentials.yml</code></li> <li><code>helm repo add capi-operator https://kubernetes-sigs.github.io/cluster-api-operator</code></li> <li><code>helm install capi-operator capi-operator/cluster-api-operator --create-namespace -n capi-operator-system --set infrastructure.proxmox.enabled=true --set configSecret.name=capmox-manager-credentials --set configSecret.namespace=default --set controlPlane.talos.enabled=true --set  bootstrap.talos.enabled=true --wait --timeout 90s</code></li> </ul>"},{"location":"ClusterAPI/#issues-and-notes","title":"Issues and notes","text":"<ul> <li>Setting IP addresses using cloudinit: <code>You will need to make sure that your VM template doesn't have Cloud-init Driver provided by Proxmox, otherwise, that will overwrite the config of CAPMOX. No need to pre-set up the Cloud-init Drive.Just use an empty CD ROM at ide0, and CAPMOX will do the job.</code></li> <li><code>clusterctl generate cluster</code> does not have an option to specify control-plane or bootstrap providers, and assumes cubeadm for both.</li> <li>clusterctl init creates a secret called <code>capmox-manager-credentials</code> in the <code>capmox-system</code> namespace.</li> <li>cluster operator creates this in the <code>proxmox-infrastructure-system namespace</code> (with empty values).</li> <li>Loading a secret manifest sets the values. Loading a cluster manifest resets them to their defaults (to be confirmed...)</li> <li>In cluster operator scenario, Proxmox expects ProxmoxCluster to include a spec.credentialsRef.</li> <li><code>clusterctl generate cluster</code> does not include spec.credentialsRef in the generated ProxmoxCluster manifest.</li> <li>The above discrepancies could relate to version differences.</li> <li>Generate cluster manifest: <code>clusterctl generate cluster cluster07 --kubernetes-version v1.31.2 --control-plane-machine-count=1 --infrastructure=proxmox:v0.7.3 &gt; cluster07.yaml</code></li> <li>NOTE: This is not useful for talos controlplane an bootstrap unless a custom template is used.</li> <li>For cluster management investigate headlamp, https://github.com/Jont828/cluster-api-visualizer/tree/main</li> </ul>"},{"location":"ClusterAPI/#references","title":"References","text":"<ul> <li>https://cluster-api.sigs.k8s.io/introduction</li> <li>https://github.com/ionos-cloud/cluster-api-provider-proxmox/tree/main</li> <li>https://github.com/siderolabs/cluster-api-bootstrap-provider-talos</li> <li>https://github.com/siderolabs/cluster-api-control-plane-provider-talos</li> <li>Discussion on flux with Cluster API Operator: https://github.com/ionos-cloud/cluster-api-provider-proxmox/issues/221</li> <li>Flux repo example: https://github.com/a7d-corp/homelab-clusters-fleet/tree/main</li> </ul>"},{"location":"ClusterChart/","title":"Cluster-Chart","text":"<p>Cluster-Chart is a helm chart for provisioning Talos kubernetes clusters on a Proxmox Hypervisor using Cluster API.</p> <p>Chart templates can be found in the cluster-chart directory.</p> <p>To publish the chart to a cloned repo:</p> <ul> <li>Create a chart binary file by running the following in the repo root directory: <code>helm package cluster-chart</code></li> <li>Create, or update the chart repo manifest: <code>helm repo index .</code></li> </ul>"},{"location":"ClusterTemplate/","title":"Using the Cluster API template","text":"<p>This section describes the use of a running cluster api management cluster.</p>"},{"location":"ClusterTemplate/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>kubectl installed</li> <li>clusterctl installed</li> <li>Running cluster api management cluster</li> </ul>"},{"location":"ClusterTemplate/#usage","title":"Usage","text":"<ul> <li>Generate variable list from cluster-template: <code>clusterctl generate yaml --list-variables --from cluster-template.yaml | grep -v Variables | sed -e 's/^  - //g' | sed -e 's/$/=/g' &gt; clusterctl-editme.yaml</code></li> <li>Edit <code>clusterctl-editme.yaml</code>, adding values, and place in <code>~/.config/cluster-api/clusterctl.yaml</code></li> <li>Generate cluster manifest: <code>clusterctl generate yaml --from cluster-template.yaml &gt; cluster09.yaml</code></li> <li>Ensure that you are using the correct cluster context for your cluster-api management cluster: <code>kubectl config get-contexts</code>, <code>kubectl config set-context ...</code></li> </ul>"},{"location":"Flux/","title":"Flux","text":""},{"location":"Flux/#introduction","title":"Introduction","text":"<p>Flux is a Kubernetes GitOps Operator. GitOps is the practice of using code repositories as the single source of truth for cluster and workload configuration.</p> <p>Flux was built by Weaveworks, and donated to the Cloud Native Computing Foundation (CNCF).</p> <p>Instrumenting a cluster with Flux requires the following:</p> <ul> <li>A git repo for the cluster definition, with its associated access username and token exported.</li> <li>Installing the flux utility on a cluster administrator workstation (with kubectl context set for the target cluster).</li> <li>Bootstrapping flux, specifying the git account, repo, branch and path.</li> </ul> <p>When the bootstrap process concludes, the GIT repo will contain the flux-system component manifests, and the target cluster will have flux-system namespace operator components installed. The flux operator with thereafter monitor the git repo for cluster configuration requirement changes, and apply the appropriate cluster adjustments.</p>"},{"location":"Flux/#dev-environment-bootstrap","title":"dev environment bootstrap","text":"<pre><code>flux bootstrap github --context=microk8s --owner=MoTTTT --repository=podzonedev-gitops --branch=main --personal --path=clusters/megalith --token-auth=true\n</code></pre>"},{"location":"Flux/#flux-operator-helm-chart-installation","title":"Flux operator helm chart installation","text":"<ul> <li>https://fluxcd.control-plane.io/operator/install/</li> <li>https://fluxcd.control-plane.io/operator/flux-sync/</li> </ul>"},{"location":"Flux/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>https://fluxcd.io/flux/cheatsheets/troubleshooting/</li> </ul>"},{"location":"Flux/#notes","title":"Notes","text":"<ul> <li>Mac OS flux Installation: <code>brew install fluxcd/tap/flux</code></li> <li>Unix install <code>curl -s https://fluxcd.io/install.sh | sudo bash</code></li> <li>Create a GITHUB token, and export username and token: <code>export GITHUB_TOKEN=&lt;token&gt;</code>; <code>export GITHUB_USER=MoTTTT</code></li> <li>Check flux and cluster: <code>flux check --pre</code></li> <li>Bootstrap: <code>flux bootstrap github --context=prod --owner=MoTTTT --repository=admin --branch=main --personal --path=clusters/prod --token-auth=true</code></li> <li>GitHub token expiry: Delete flux secret, and bootstrap again with new token in context.</li> </ul>"},{"location":"Flux/#resources-created-by-flux","title":"Resources created by flux","text":"<p>Can be seen on delete: <code>flux uninstall</code></p> <pre><code>martincolley@Dolmen:~/workspace/admin/clusters/ukdev$ flux uninstall\nAre you sure you want to delete Flux and its custom resource definitions: y\n\u25ba deleting components in flux-system namespace\n\u2714 Deployment/flux-system/helm-controller deleted \n\u2714 Deployment/flux-system/kustomize-controller deleted \n\u2714 Deployment/flux-system/notification-controller deleted \n\u2714 Deployment/flux-system/source-controller deleted \n\u2714 Service/flux-system/notification-controller deleted \n\u2714 Service/flux-system/source-controller deleted \n\u2714 Service/flux-system/webhook-receiver deleted \n\u2714 NetworkPolicy/flux-system/allow-egress deleted \n\u2714 NetworkPolicy/flux-system/allow-scraping deleted \n\u2714 NetworkPolicy/flux-system/allow-webhooks deleted \n\u2714 ServiceAccount/flux-system/helm-controller deleted \n\u2714 ServiceAccount/flux-system/kustomize-controller deleted \n\u2714 ServiceAccount/flux-system/notification-controller deleted \n\u2714 ServiceAccount/flux-system/source-controller deleted \n\u2714 ClusterRole/crd-controller-flux-system deleted \n\u2714 ClusterRole/flux-edit-flux-system deleted \n\u2714 ClusterRole/flux-view-flux-system deleted \n\u2714 ClusterRoleBinding/cluster-reconciler-flux-system deleted \n\u2714 ClusterRoleBinding/crd-controller-flux-system deleted \n\u25ba deleting toolkit.fluxcd.io finalizers in all namespaces\n\u2714 GitRepository/flux-system/flux-system finalizers deleted \n\u2714 Kustomization/flux-system/flux-system finalizers deleted \n\u25ba deleting toolkit.fluxcd.io custom resource definitions\n\u2714 CustomResourceDefinition/alerts.notification.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/buckets.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/gitrepositories.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/helmcharts.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/helmreleases.helm.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/helmrepositories.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/kustomizations.kustomize.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/ocirepositories.source.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/providers.notification.toolkit.fluxcd.io deleted \n\u2714 CustomResourceDefinition/receivers.notification.toolkit.fluxcd.io deleted \n\u2714 Namespace/flux-system deleted \n\u2714 uninstall finished\n</code></pre>"},{"location":"Harbor/","title":"Harbor Installation","text":""},{"location":"Harbor/#container-image-caching","title":"Container image caching","text":"<p>Over 6 GB of in container images are pulled regularly pulled by each kubernetes node, even for a cluster with a small workload. Many of these containers are identical between cluster nodes, but are typically each pulled individually from public image repositories.</p> <p>If clusters are regularly destroyed and rebuilt, together with their workload, as is required in a platform lab, image pull duration becomes the largest factor in cluster provisioning time. Rate limiting on public repositories also becomes a factor.</p> <p>Including a cluster caching repository on the cluster would significantly reduce per cluster provisioning time.</p> <p>Including a caching repository at an infrastructure level would reduce cluster provisioning time further, and would provide the best mitigation against rate limiting on public repositories.</p> <p>Harbor is used here to achieve caching, improving cluster provisioning time, and avoiding rate limiting risks.</p> <p>Harbor will be installed in a dedicated VM in the Proxmox cluster.</p> <p>On a proxmox host:</p> <ul> <li>Download no-cloud ubuntu image:  <li>Create a VM with VM ID 1000 with no OS and Hard Disk config, without starting it.</li> <li>Import the image into the VM: <code>qm importdisk 1000 noble-server-cloudimg-amd64.img pool1</code> (pool1 may be local or local-lvm etc)</li> <li>Add a cloudinit drive to the VM, and set up access controls and IP address. In this case we use <code>192.168.4.100</code></li> <li>Configure boot order, resize the VM hard drive to accommodate image cache. In this case we use 55 GB.</li> <li>Start the VM, log in and install docker https://docs.docker.com/engine/install/ubuntu/.</li> <li>Install harbor with quick install script https://goharbor.io/docs/2.0.0/install-config/quick-install-script/</li> <li>Configure at least <code>gcr.io</code>, <code>registry.k8s.io</code>, <code>ghrc.io</code>, and <code>hub.docker.com</code> Registries, and proxied Projects for each.</li> <li>In the cluster09 workload, there are images from the following that can also be proxied: <code>quay.io</code>, <code>cr.fluentbit.io</code>.</li> <li>Add <code>machine.registries.mirrors</code> entries to tolos configurations, in the following form, for each Project created:</li> <pre><code>machine:\n  registries:\n    mirrors:\n      docker.io:\n        endpoints:\n          - http://192.168.4.100/v2/proxy-docker.io\n        overridePath: true\n</code></pre> <ul> <li>Add <code>machine.registries.config</code> entry for the credentials set up for harbor. By default this will be:</li> </ul> <pre><code>machine:\n  registries:\n    config:\n      192.168.4.100:\n        auth:\n          username: admin\n          password: Harbor12345\n</code></pre>"},{"location":"Harbor/#references","title":"References","text":"<ul> <li>https://docs.siderolabs.com/talos/v1.10/configure-your-talos-cluster/images-container-runtime/pull-through-cache</li> </ul>"},{"location":"Manifests/","title":"Manifests","text":"<p>During talos node configuration, a set of kubernetes manifests are loaded.</p> <p>These are cached in the Cluster09 repo, and served from the web server in the management cluster.</p> <p>Their configuration can be found in the Cluster template, in the cluster.extraManifests array.</p> manifest name Source kubelet-serving-cert-approver.yaml https://raw.githubusercontent.com/alex1989hu/kubelet-serving-cert-approver/main/deploy/standalone-install.yaml metrics-server.yaml https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml piraeus-operator.yaml https://github.com/piraeusdatastore/piraeus-operator/releases/latest/download/manifest.yaml gateway-api.yaml https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.3.0/standard-install.yaml cilium.yaml helm template \\ cilium \\ cilium/cilium \\ --version 1.17.4 \\ --set hubble.relay.enabled=true \\ --set hubble.ui.enabled=true \\ --set ingressController.enabled=true \\ --set ingressController.loadbalancerMode=shared \\ --set ingressController.default=true \\ --set l2announcements.enabled=true \\ --set l2announcements.leaseDuration=3s \\ --set l2announcements.leaseRenewDeadline=1s \\ --set l2announcements.leaseRetryPeriod=200ms \\ --set loadBalancerIPs.enable=true \\ --set gatewayAPI.enabled=true \\ --set loadBalancer.l7.backend=envoy \\ --namespace kube-system \\ --set ipam.mode=kubernetes \\ --set kubeProxyReplacement=true \\ --set securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\ --set securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\ --set cgroup.autoMount.enabled=false \\ --set cgroup.hostRoot=/sys/fs/cgroup \\ --set k8sServiceHost=localhost \\ --set k8sServicePort=7445 &gt; cilium.yaml <p>```</p>"},{"location":"Network/","title":"Network Architecture","text":""},{"location":"Network/#proxmox-hypervisor-running-on-a-poweredge-r730","title":"Proxmox Hypervisor running on a PowerEdge R730","text":"<ul> <li>The R730 runs on an isolated network, with the bastion server, having two network interfaces, forming a glasshouse function.</li> <li>Web traffic from the internet to any supported domains are port forwarded to the bastion server</li> <li>The bastion server provides internet gateway services to the isolated network.</li> <li>Various services in the isolated network are made available via the bastion server for local access.</li> </ul> <pre><code>---\ntitle: venus.podzone.net\n---\ngraph TD\nInternet  -- :80 :443 --&gt; router -- :80 :443 --&gt; bastion\nworkstation --&gt; bastion\nbastion -- 192.168.4.50 --&gt; proxmoxAPI\nbastion -- 192.168.4.198 --&gt; ingress \nbastion -- 192.168.4.199 --&gt; g1[gateway]\nbastion -- 192.168.4.209 --&gt; g2[gateway]\nbastion -- 192.168.4.200 --&gt; c2[k8s API]\nbastion -- 192.168.4.247 --&gt; iDRAC\nbastion -- 192.168.4.190 --&gt; c1[k8s API]\nsubgraph venus\n    subgraph kubernetes cluster09\n      ingress --&gt; service1\n      ingress --&gt; serviceN\n      g1 --&gt; httpsroute1\n      g1 --&gt; httpsrouteN\n      c1\n    end\n    subgraph Cluster Management cluster\n      c2 --&gt;  CAPI\n      g2 --&gt; manifestServer\n      g2 --&gt; imageCache\n      c2\n    end\n  proxmoxAPI\n  iDRAC\n  Disk((4TB DISK))\nend \n</code></pre>"},{"location":"Network/#provisioning","title":"Provisioning","text":"<ul> <li>https://github.com/christensenjairus/ClusterCreator</li> <li>https://cyber-engine.com/blog/2024/06/25/k8s-on-proxmox-using-clustercreator/</li> <li>https://github.com/DushanthaS/kubernetes-the-hard-way-on-proxmox</li> <li>https://github.com/siderolabs/image-factory</li> <li>https://blog.stonegarden.dev/articles/2024/08/talos-proxmox-tofu/</li> <li>https://github.com/rgl/terraform-proxmox-talos</li> <li>https://olav.ninja/talos-cluster-on-proxmox-with-terraform</li> <li>https://github.com/kubernetes-sigs/kubespray</li> <li>https://kubespray.io/</li> <li>https://blog.andreasm.io/2024/01/15/proxmox-with-opentofu-kubespray-and-kubernetes/</li> <li>https://medium.com/@abhigyan.dwivedi_58961/creating-a-kvm-kubernetes-cluster-with-vagrant-kubespray-and-ansible-a-idiot-resistant-guide-2f3727ce7039</li> </ul>"},{"location":"Network/#monitoring","title":"Monitoring","text":"<ul> <li>https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack</li> <li>https://github.com/prometheus-operator/kube-prometheus</li> <li>Network Monitoring: https://docs.cilium.io/en/stable/overview/intro/</li> <li>Kubernetes Document Generation: https://github.com/philippemerle/KubeDiagrams</li> <li>Document Generation: https://www.graphviz.org/</li> </ul>"},{"location":"Network/#monitoring-hardware","title":"Monitoring hardware","text":"<ul> <li>IPMI Tool suite: https://www.gnu.org/software/freeipmi/</li> <li>Dell idrac exporter: https://github.com/galexrt/dellhw_exporter</li> <li>Dell Redfish idrac exporter: https://github.com/mrlhansen/idrac_exporter</li> <li>Redfish idrac exporter: https://github.com/jenningsloy318/redfish_exporter</li> <li>Prometheus ipmi exporter: https://github.com/prometheus-community/ipmi_exporter</li> <li>SNMP Exporter: https://github.com/prometheus/snmp_exporter</li> <li>Helm chart for SNMP Exporter: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-snmp-exporter</li> <li>Node-Exporter ipmitool collector: https://github.com/prometheus-community/node-exporter-textfile-collector-scripts/blob/master/ipmitool</li> <li>Grafana Dashboard for idrac: https://grafana.com/grafana/dashboards/13177-ipmi-for-prometheus/</li> </ul>"},{"location":"Network/#networking","title":"Networking","text":"<ul> <li>Creating template files: https://github.com/trfore/proxmox-template-scripts?tab=readme-ov-file</li> <li>For Bastion: IP Tables Load Balancer: https://github.com/muzahid-c/iptables-loadbalancer</li> </ul>"},{"location":"Network/#cloud-init","title":"Cloud init","text":"<ul> <li>https://austinsnerdythings.com/2021/08/30/how-to-create-a-proxmox-ubuntu-cloud-init-image/</li> </ul>"},{"location":"Network/#storage","title":"Storage","text":"<ul> <li>https://github.com/LINBIT/linstor-server</li> <li>https://linbit.com/drbd/</li> <li>https://github.com/piraeusdatastore/piraeus-operator</li> <li>https://syncthing.net/</li> <li>https://github.com/sergelogvinov/proxmox-csi-plugin</li> </ul>"},{"location":"Planning/","title":"Planning","text":""},{"location":"Planning/#backlog","title":"Backlog","text":"<ul> <li>[ ] Move Loadbalancer manifest to cluster config</li> <li>[ ] Log visibility: Send service and kernel logs to fluentbit: Set fluentbit service up as a NodePort</li> <li>[ ] Managed secrets: <code>talosctl gen secrets --from-controlplane-config controlplane.yaml -o secrets.yaml</code></li> <li>[ ] Observability: tracing with Jaeger</li> <li>[ ] Observability / Security: Falco (Security monitoring)</li> <li>[ ] Security: Kyverno (Policy as code)</li> <li>[ ] Security: Keycloak</li> <li>[ ] OpenTelemetry</li> <li>[ ] Evaluate TALM: <code>https://github.com/cozystack/talm</code></li> <li>[ ] Evaluate authentik</li> <li>[ ] Automated provisioning: <code>https://cozystack.io/</code></li> <li>[ ] Switch to Gateway API: https://medium.com/@martin.hodges/why-do-i-need-an-api-gateway-on-a-kubernetes-cluster-c70f15da836c</li> <li>[ ] Split out storage network: <code>https://cozystack.io/docs/operations/storage/dedicated-network/</code></li> <li>[ ] Refactor ingresses with URL prefixes</li> <li>[ ] Security SSO: Oauth2-proxy</li> <li>[ ] SOPS with age https://fluxcd.io/flux/guides/mozilla-sops/#encrypting-secrets-using-age; https://pkg.go.dev/filippo.io/age</li> <li>[ ] Investigate kubewall https://github.com/kubewall/kubewall</li> <li>[ ] Investigate Kubeshark: Network traffic analyser https://github.com/kubeshark/kubeshark</li> <li>[ ] Investigate Pixie https://px.dev/</li> <li>[ ] Investigate Jaeger https://github.com/jaegertracing/jaeger-operator</li> <li>[ ] Investigate OpenTelemetry: https://opentelemetry.io/; https://github.com/open-telemetry/opentelemetry-operator</li> <li>[ ] Investigate Kubeflow (AI Tool ecosystem): https://www.kubeflow.org/</li> <li>[ ] Investigate: For VMs in k8s, see kubevirt</li> <li>[ ] Investigate: For flux git access secret https://fluxcd.io/flux/cmd/flux_create_secret_git/</li> </ul>"},{"location":"Planning/#cluster09-changes","title":"Cluster09 changes","text":"<ul> <li>[X] Greenfields Bootstrap {Terraform templates and Talos config, and documents for Management Cluster}</li> <li>[X] Manual CAPI provider installation</li> <li>[X] Manual Management cluster gitops bootstrap</li> <li>[ ] Managed cluster dependencies served by Management cluster workload</li> <li>[ ] Cluster dependency: Image cache</li> <li>[ ] Cluster dependency: Manifest server</li> <li>[ ] Cluster dependency: Documentation</li> </ul>"},{"location":"Planning/#cluster08-changes","title":"Cluster08 changes","text":"<ul> <li>[X] Template for <code>clusterctl generate cluster</code>: <code>clusterctl generate yaml  --from cluster-template.yaml &gt; cluster08.yaml</code></li> <li>[X] Gateway API for Hubble: https://blog.grosdouli.dev/blog/cilium-gateway-api-cert-manager-let's-encrypt</li> </ul>"},{"location":"Planning/#cluster07-changes","title":"Cluster07 changes","text":"<ul> <li>[X] Cluster API: clusterctl for provisioning (cluster api operator rolled back)</li> <li>[X] Final talos config {VIP, mirror registry (harbor), drbd, sysctls, certSANs, cilium, talos-cloud-controller-manager}</li> <li>[X] Move extraManifests to local httpd {kubelet-serving-cert-approver, metrics-server, piraeus-operator, gateway-api, cilium}</li> <li>[X] Reintroduce support for cilium, drdb</li> <li>[ ] Fix mv naming (what was this?)</li> </ul>"},{"location":"Planning/#cluster06-changes","title":"Cluster06 changes","text":"<ul> <li>[X] Switch to Cluster API for provisioning, with simplified talos config</li> </ul>"},{"location":"Planning/#cluster05-changes","title":"Cluster05 changes","text":"<ul> <li>[X] Use TALM for talos configuration: Cancelled</li> <li>[X] Reduce disk to 40 GB: Reduce storage startup time?: {8 CPU; 16 GB RAM; 40 GB Disk}</li> </ul>"},{"location":"Planning/#cluster04-changes","title":"Cluster04 changes","text":"<ul> <li>[X] Dependency: Harbor</li> <li>[X] Configure machine.registry (Harbour) as docker caching repository</li> <li>[X] Workload: Prometheus and Grafana</li> <li>[X] Cache Talos startup image on bastion server: <code>https://factory.talos.dev/image/ed7716909fb764e0c322ab43dd20918e30cf8ffa3914ba3fa229afec9efe4d84/v1.10.2/nocloud-amd64.iso</code></li> <li>[X] Helm chart dependsOn: Fix for dashboards index creation failure</li> <li>[X] Split Opensearch roles</li> <li>[X] Distribute ingresses</li> <li>[X] Add worker node</li> <li>[X] Workload: Keycloak</li> </ul>"},{"location":"Planning/#cluster03-changes","title":"Cluster03 Changes","text":"<ul> <li>[X] Workload: Radio station (ingress, nfs, storageclass)</li> <li>[X] Dependency: NFS</li> <li>[X] Add log visibility: Machine definition pre-requisites</li> <li>[X] Increase boot size to accommodate linstore pool usage</li> <li>[X] VM dimensions {8 CPU; 16 GB RAM; 80 GB Disk}</li> </ul>"},{"location":"Planning/#cluster02-changes","title":"Cluster02 Changes","text":"<ul> <li>[X] Workload: OpenSearch with Dashboard</li> <li>[X] Workload: fluentbit {collect logs from kubernetes containers}</li> <li>[X] Move affinity controller installation to after linstor is stable</li> <li>[X] Fix: Shared ingress</li> <li>[X] Adjust resource allocations</li> <li>[X] Abstraction of controlplane and worker resource dimensions</li> <li>[X] Abstraction of Talos version</li> <li>[X] Kustomization patch for Ingres load balancer IP Pool</li> <li>[X] Support for internet proxy:</li> <li>[X] VM dimensions {8 CPU; 16 GB RAM; 40 GB Disk}</li> </ul>"},{"location":"Planning/#generation-01","title":"Generation 01","text":"<ul> <li>VM dimensions {4 CPU; 8 GB RAM; 20 GB Disk</li> </ul>"},{"location":"Planning/#generation-01-cluster08","title":"Generation 01: Cluster08","text":"<p>Using CreateVMDefinitions.sh process template files to:</p> <ul> <li>Generate terraform vm definitions, provider definition, and talos installation media</li> <li>Generate Cilium manifests and appends them to talos patch file</li> <li>Generate createCluster.sh, which generates (patched) talos configs and apply then to the machines in the cluster</li> <li>createCluster.sh also generates command snippets for talos bootstrap, kube config, flux bootstrap, dashboard access etc</li> </ul>"},{"location":"Planning/#generation-01-cluster05","title":"Generation 01: Cluster05","text":"<ul> <li><code>talosctl gen config cluster05 https://192.168.4.114:6443 --config-patch @patch.yaml</code></li> <li><code>talosctl apply-config --insecure --nodes 192.168.4.114 --file controlplane.yaml</code></li> <li><code>talosctl apply-config --insecure --nodes 192.168.4.115 --file worker.yaml</code></li> <li><code>talosctl bootstrap --nodes 192.168.4.114 --endpoints 192.168.4.114 --talosconfig=./talosconfig</code></li> <li><code>talosctl kubeconfig --nodes 192.168.4.114 --endpoints 192.168.4.114 --talosconfig=./talosconfig</code></li> <li><code>kubectl apply -f cilium.yaml</code></li> <li><code>kubectl apply --server-side -k \"https://github.com/piraeusdatastore/piraeus-operator/config/default?ref=v2.8.1\"</code></li> <li>Create Github token, export as GITHUB_TOKEN, with GITHUB_USER</li> <li><code>flux bootstrap github --context=admin@cluster05 --owner=MoTTTT --repository=venus --branch=main --personal --path=clusters/cluster05 --token-auth=true</code></li> <li>To create flux resource for piraeus: <code>flux create source git piraeus --url=https://github.com/piraeusdatastore/piraeus-operator/config/default --tag=\"v2.8.1\"</code></li> </ul>"},{"location":"References/","title":"References","text":""},{"location":"References/#provisioning","title":"Provisioning","text":"<ul> <li>https://github.com/christensenjairus/ClusterCreator</li> <li>https://cyber-engine.com/blog/2024/06/25/k8s-on-proxmox-using-clustercreator/</li> <li>https://github.com/DushanthaS/kubernetes-the-hard-way-on-proxmox</li> <li>https://github.com/siderolabs/image-factory</li> <li>https://blog.stonegarden.dev/articles/2024/08/talos-proxmox-tofu/</li> <li>https://github.com/rgl/terraform-proxmox-talos</li> <li>https://olav.ninja/talos-cluster-on-proxmox-with-terraform</li> <li>https://github.com/kubernetes-sigs/kubespray</li> <li>https://kubespray.io/</li> <li>https://blog.andreasm.io/2024/01/15/proxmox-with-opentofu-kubespray-and-kubernetes/</li> <li>https://medium.com/@abhigyan.dwivedi_58961/creating-a-kvm-kubernetes-cluster-with-vagrant-kubespray-and-ansible-a-idiot-resistant-guide-2f3727ce7039</li> </ul>"},{"location":"References/#monitoring","title":"Monitoring","text":"<ul> <li>https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack</li> <li>https://github.com/prometheus-operator/kube-prometheus</li> <li>Network Monitoring: https://docs.cilium.io/en/stable/overview/intro/</li> <li>Kubernetes Document Generation: https://github.com/philippemerle/KubeDiagrams</li> <li>Document Generation: https://www.graphviz.org/</li> </ul>"},{"location":"References/#monitoring-hardware","title":"Monitoring hardware","text":"<ul> <li>IPMI Tool suite: https://www.gnu.org/software/freeipmi/</li> <li>Dell idrac exporter: https://github.com/galexrt/dellhw_exporter</li> <li>Dell Redfish idrac exporter: https://github.com/mrlhansen/idrac_exporter</li> <li>Redfish idrac exporter: https://github.com/jenningsloy318/redfish_exporter</li> <li>Prometheus ipmi exporter: https://github.com/prometheus-community/ipmi_exporter</li> <li>SNMP Exporter: https://github.com/prometheus/snmp_exporter</li> <li>Helm chart for SNMP Exporter: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-snmp-exporter</li> <li>Node-Exporter ipmitool collector: https://github.com/prometheus-community/node-exporter-textfile-collector-scripts/blob/master/ipmitool</li> <li>Grafana Dashboard for idrac: https://grafana.com/grafana/dashboards/13177-ipmi-for-prometheus/</li> </ul>"},{"location":"References/#networking","title":"Networking","text":"<ul> <li>Creating template files: https://github.com/trfore/proxmox-template-scripts?tab=readme-ov-file</li> <li>For Bastion: IP Tables Load Balancer: https://github.com/muzahid-c/iptables-loadbalancer</li> </ul>"},{"location":"References/#cloud-init","title":"Cloud init","text":"<ul> <li>https://austinsnerdythings.com/2021/08/30/how-to-create-a-proxmox-ubuntu-cloud-init-image/</li> </ul>"},{"location":"References/#storage","title":"Storage","text":"<ul> <li>https://github.com/LINBIT/linstor-server</li> <li>https://linbit.com/drbd/</li> <li>https://github.com/piraeusdatastore/piraeus-operator</li> <li>https://syncthing.net/</li> <li>https://github.com/sergelogvinov/proxmox-csi-plugin</li> </ul>"},{"location":"References/#cluster-components","title":"Cluster components","text":"<ul> <li>https://github.com/adampetrovic/home-ops/tree/main</li> <li>https://www.cloudraft.io/blog/making-kubernetes-simple-with-talos</li> <li>https://blog.devops.dev/talos-os-raspberry-fc5f327b7026</li> <li>https://kevinholditch.co.uk/2023/10/21/creating-a-kubernetes-cluster-using-talos-linux-on-xen-orchestra/</li> </ul>"},{"location":"References/#talos","title":"Talos","text":"<ul> <li>https://www.talos.dev/v1.8/introduction/prodnotes/</li> <li>https://factory.talos.dev/?arch=amd64&amp;cmdline-set=true&amp;extensions=-&amp;extensions=siderolabs%2Fiscsi-tools&amp;extensions=siderolabs%2Fqemu-guest-agent&amp;platform=nocloud&amp;target=cloud&amp;version=1.8.2</li> <li>https://www.talos.dev/v1.8/talos-guides/install/virtualized-platforms/proxmox/</li> <li>https://www.talos.dev/v1.8/talos-guides/install/cloud-platforms/nocloud/</li> <li>https://blog.devops.dev/talos-os-raspberry-fc5f327b7026</li> <li>https://www.civo.com/blog/calico-vs-flannel-vs-cilium</li> <li>https://cilium.io/</li> <li>https://www.cloudraft.io/blog/making-kubernetes-simple-with-talos</li> <li>https://github.com/adampetrovic/home-ops/blob/main/kubernetes/bootstrap/talos/talconfig.yaml</li> <li>KaaS GitOps: https://github.com/kubebn/talos-proxmox-kaas</li> <li>PXE: https://www.talos.dev/v1.8/talos-guides/install/bare-metal-platforms/pxe/</li> <li>Options: https://www.civo.com/blog/calico-vs-flannel-vs-cilium</li> <li>https://github.com/cilium/cilium</li> <li>https://github.com/cilium/cilium/blob/v1.18.1/install/kubernetes/cilium/values.yaml#L992</li> </ul>"},{"location":"Storage/","title":"Storage","text":""},{"location":"Storage/#zfs-storage-configuration-on-hypervisor","title":"ZFS storage configuration on hypervisor","text":"<ul> <li>Hypervisor storage resources: 8 X 600GB SAS Disk, configured in IT mode</li> <li>1 Disk assigned to hypervisor (no redundancy, SPOF)</li> <li>7 disks configured in raid 0 array (Any of 7 disks SPOF)</li> <li>Optimising for disk utilisation, and speed</li> <li>Optimising for cluster build repeatability</li> </ul>"},{"location":"Storage/#example-raid-0-configuration","title":"Example Raid 0 configuration","text":"<ul> <li>List available volumes: <code>ls -la /dev/disk/by-id</code></li> <li>Create pool: <code>zpool create pool1 wwn-0x5000cca05633e2d0 ... wwn-0x500003930821ed54</code></li> <li>Add pool: <code>pvesm add zfspool pool1 -pool pool1</code></li> </ul>"},{"location":"Storage/#distributed-storage","title":"Distributed storage","text":"<ul> <li>Using in-cluster Linstor</li> </ul>"},{"location":"Storage/#linstor-installation","title":"Linstor installation","text":"<ul> <li>Check talos extensions: <code>talosctl --talosconfig=./talosconfig --endpoints 192.168.4.114 --nodes 192.168.4.114 get extensions</code></li> <li>Check talos loaded modules: <code>talosctl --talosconfig=./talosconfig --endpoints 192.168.4.114 --nodes 192.168.4.114 read /proc/modules</code></li> <li>Check talos drbd config: <code>talosctl --talosconfig=./talosconfig --endpoints 192.168.4.114 --nodes 192.168.4.114 read /sys/module/drbd/parameters/usermode_helper</code></li> <li><code>kubectl apply --server-side -k \"https://github.com/piraeusdatastore/piraeus-operator/config/default?ref=v2.8.1\"</code></li> </ul> <p>Apply overrides:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: talos-loader-override\nspec:\n  podTemplate:\n    spec:\n      initContainers:\n        - name: drbd-shutdown-guard\n          $patch: delete\n        - name: drbd-module-loader\n          $patch: delete\n      volumes:\n        - name: run-systemd-system\n          $patch: delete\n        - name: run-drbd-shutdown-guard\n          $patch: delete\n        - name: systemd-bus-socket\n          $patch: delete\n        - name: lib-modules\n          $patch: delete\n        - name: usr-src\n          $patch: delete\n        - name: etc-lvm-backup\n          hostPath:\n            path: /var/etc/lvm/backup\n            type: DirectoryOrCreate\n        - name: etc-lvm-archive\n          hostPath:\n            path: /var/etc/lvm/archive\n            type: DirectoryOrCreate\n</code></pre> <p>Create Linstor cluster:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec: {}\n</code></pre> <p>Check node list: <code>kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor node list</code></p> <p>Configure storage:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: storage-pool\nspec:\n  storagePools:\n    - name: pool1\n      fileThinPool:\n        directory: /var/lib/piraeus-datastore/pool1\n</code></pre> <p>Check storage pool: <code>kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor storage-pool list</code></p> <p>Create storage class:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: piraeus-storage\nprovisioner: linstor.csi.linbit.com\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  linstor.csi.linbit.com/storagePool: pool1\n</code></pre> <p>Replicated storage class:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: piraeus-storage-replicated\nprovisioner: linstor.csi.linbit.com\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  linstor.csi.linbit.com/storagePool: pool1\n  linstor.csi.linbit.com/placementCount: \"2\"\n</code></pre> <p>Snapshots: https://piraeus.io/docs/stable/tutorial/snapshots</p> <p>Deploy snapshot controller</p> <pre><code>kubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//client/config/crd\nkubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//deploy/kubernetes/snapshot-controller\n</code></pre> <p>Create SnapshotClass</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: piraeus-snapshots\ndriver: linstor.csi.linbit.com\ndeletionPolicy: Delete\n</code></pre> <p>Create VolumeSnapshot</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: data-volume-snapshot-1\nspec:\n  volumeSnapshotClassName: piraeus-snapshots\n  source:\n    persistentVolumeClaimName: data-volume\n</code></pre> <p>Affinity controller:</p> <p><code>helm repo add piraeus-charts https://piraeus.io/helm-charts/</code> <code>helm install linstor-affinity-controller piraeus-charts/linstor-affinity-controller</code></p> <p>Monitoring with Prometheus operator:</p> <p>Install prometheus:</p> <p><code>$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts</code></p> <pre><code>helm install --create-namespace -n monitoring prometheus prometheus-community/kube-prometheus-stack \\\n  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\\n  --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \\\n  --set prometheus.prometheusSpec.ruleSelectorNilUsesHelmValues=false\n</code></pre> <p>Add monitoring and alert rules for piraeus:</p> <pre><code>kubectl apply --server-side -n piraeus-datastore -k \"https://github.com/piraeusdatastore/piraeus-operator//config/extras/monitoring?ref=v2\"\n</code></pre>"},{"location":"Storage/#switching-to-gitops","title":"Switching to gitops","text":"<ul> <li><code>flux create source helm piraeus-charts  --url=https://prometheus-community.github.io/helm-charts --interval=10m --export</code></li> <li><code>flux create hr affinity-controller  --interval=10m  --source=HelmRepository/piraeus-charts  --chart=linstor-affinity-controller --export</code></li> <li>See gitops-infra for manifests.</li> </ul>"},{"location":"Storage/#references","title":"References","text":"<ul> <li>Reference: https://piraeus.io/docs/stable/tutorial/get-started</li> </ul>"},{"location":"Talos/","title":"Talos","text":""},{"location":"Talos/#log-visibility","title":"Log Visibility","text":""},{"location":"Talos/#talos-system-level-interrogation","title":"Talos system level interrogation","text":"<ul> <li>System logs and status logs: <code>talosctl -n 192.168.4.121 dmesg</code>, <code>talosctl -n 192.168.4.121 services</code>, <code>talosctl -n 192.168.4.121 logs machined</code></li> <li>Container list: <code>talosctl -n 192.168.4.121 containers -k</code></li> <li>Container logs: <code>talosctl -n 192.168.4.121 logs -k kube-system/cilium-cxrpb:cilium-agent:904e8a41ccff</code></li> </ul>"},{"location":"Talos/#aggregating-logs","title":"Aggregating logs","text":"<ul> <li>Using opensearch for search index, and opensearch-dashboard for visibility</li> <li>Using fluentbit for log collection</li> </ul>"},{"location":"Talos/#manual-patch-application","title":"Manual patch application","text":"<ul> <li><code>talosctl gen config cluster05 https://192.168.4.114:6443 --config-patch @patch.yaml</code></li> <li><code>talosctl apply-config --insecure --nodes 192.168.4.114 --file controlplane.yaml</code></li> <li><code>talosctl apply-config --insecure --nodes 192.168.4.115 --file worker.yaml</code></li> <li><code>talosctl bootstrap --nodes 192.168.4.114 --endpoints 192.168.4.114 --talosconfig=./talosconfig</code></li> <li><code>talosctl kubeconfig --nodes 192.168.4.114 --endpoints 192.168.4.114 --talosconfig=./talosconfig</code></li> <li><code>kubectl apply -f cilium.yaml</code></li> <li><code>kubectl apply --server-side -k \"https://github.com/piraeusdatastore/piraeus-operator/config/default?ref=v2.8.1\"</code></li> <li>Create Github token, export as GITHUB_TOKEN, with GITHUB_USER</li> <li> <p><code>flux bootstrap github --context=admin@cluster05 --owner=MoTTTT --repository=venus --branch=main --personal --path=clusters/cluster05 --token-auth=true</code></p> </li> <li> <p>Get disk info: <code>talosctl -n 192.168.4.100,192.168.4.101,192.168.4.103 disks</code></p> </li> </ul>"},{"location":"Talos/#talos-image-selection","title":"Talos image selection","text":"<ul> <li>QEMU guest agent</li> <li>DRDB, required for LinStore</li> </ul> <pre><code>customization:\n    systemExtensions:\n        officialExtensions:\n            - siderolabs/drbd\n            - siderolabs/qemu-guest-agent\n</code></pre>"},{"location":"Talos/#general-talos-patch-snippets","title":"General Talos patch snippets","text":"<p>A Talos patch set that meets the requirements of the provisioning system, and opinionated CNI and CSI selection, specifically for Cilium and LinStore is configured by default. These choices can be modified in the helm-chart values.yaml file used during provisioning.</p> <p>Because Proxmox:</p> <ul> <li>Set Talos machine install disk to default Proxmox boot disk device: <code>machine.install.disk</code></li> </ul> <p>Because Cilium:</p> <ul> <li>Flannel CNI installs by default. For enterprise CNI features, installing Cilium</li> <li>Disable Talos CNI support, in preparation for Cilium installation: <code>cluster.network.cni.name</code>, and <code>cluster.proxy.disabled</code></li> <li>Load cilium manifest at cluster configuration time (the cluster cannot complete startup without this manifest):<code>cluster.extraManifests</code></li> </ul> <p>Because Talos:</p> <ul> <li>Configure talos boot image with: <code>machine.install.image</code></li> <li>To set a Virtual IP address for the kubernetes api server: <code>machine.network.interfaces[0].vip.ip</code></li> </ul> <p>Because LinStore:</p> <ul> <li>Distributed Storage: Enable drdb for linstor and piraeus operator: <code>machine.kernel.modules</code></li> </ul> <p>Depending on network environment:</p> <ul> <li>Internet proxy configuration: <code>machine.env.http_proxy</code> and <code>machine.env.https_proxy</code></li> <li>For the kubernetes cluster api certificate to be trusted off-net (e.g. via bastion, or routed in from the internet): <code>cluster.apiServer.certSANs</code></li> </ul> <p>Depending on required target cluster workloads:</p> <ul> <li>For single node clusters: <code>cluster.allowSchedulingOnControlPlanes</code></li> <li>Sysctls configuration example: <code>machine.sysctls.vm.max_map_count</code>, required for OpenSearch cluster workloads</li> <li>Additional manifests to install at cluster startup time (before GitOps bootstrap): <code>cluster.extraManifests</code></li> <li>Use <code>cluster.extraManifests</code> to install Flux, and a FluxInstance for the target cluster to GitOps bootstrap from.</li> </ul> <p>Because CAPI:</p> <ul> <li>For CAPI Talos agent install: <code>cluster.externalCloudProvider</code>, and configuration: <code>machine.kubelet.extraArgs</code></li> <li>For CAPI Talos agent to access kubernetes cluster api: <code>machine.features.kubernetesTalosAPIAccess</code></li> </ul> <p>Machine patch:</p> <pre><code>machine:\n  install:\n    disk: /dev/vda\n    image: {{ .Values.cluster.image }}\n  sysctls:\n    vm.max_map_count: 262144\n  kernel:\n    modules:\n      - name: drbd\n        parameters:\n          - usermode_helper=disabled\n      - name: drbd_transport_tcp\n  kubelet:\n    extraArgs:\n      rotate-server-certificates: true\n      cloud-provider: external\n</code></pre> <p>Controlplane patch:</p> <pre><code>machine:\n network:\n   interfaces:\n     - deviceSelector:\n         physical: true\n       vip:\n         ip: {{ .Values.controlplane.endpoint_ip }}\n install:\n   disk: /dev/vda\n   image: {{ .Values.cluster.image }}\n kernel:\n   modules:\n     - name: drbd\n       parameters:\n         - usermode_helper=disabled\n     - name: drbd_transport_tcp\n kubelet:\n   extraArgs:\n     rotate-server-certificates: true\n     cloud-provider: external\n features:\n   kubernetesTalosAPIAccess:\n     enabled: true\n     allowedRoles:\n       - os:reader\n     allowedKubernetesNamespaces:\n       - kube-system\n sysctls:\n   vm.max_map_count: 262144\ncluster:\n  apiServer:\n    certSANs:\n      - {{ .Values.network.bastion_host_endpoint_ip }}\n      - {{ .Values.network.bastion_host }}\n  network:\n    cni:\n      name: none\n  proxy:\n    disabled: true\n  {{- with .Values.controlplane.extra_manifests }}\n  extraManifests:\n  {{- toYaml . | nindent 14 }}\n  {{- end }}\n  externalCloudProvider:\n    enabled: true\n    manifests:\n      - https://raw.githubusercontent.com/siderolabs/talos-cloud-controller-manager/main/docs/deploy/cloud-controller-manager.yml\n</code></pre>"},{"location":"Talos/#talosctl-client-configuration","title":"Talosctl client configuration","text":"<ul> <li>Mac: <code>brew install siderolabs/tap/talosctl</code></li> <li>On unix, need to first install brew dependencies: <code>sudo apt-get install build-essential procps curl file git</code></li> <li>Then need to install brew: <code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"</code></li> <li>Add brew to path: <code>alias brew=\"/home/linuxbrew/.linuxbrew/Homebrew/bin/brew\"</code></li> <li>Then talosctl: <code>brew install siderolabs/tap/talosctl</code></li> <li><code>alias talosctl=\"/home/linuxbrew/.linuxbrew/Homebrew/Cellar/talosctl/1.9.1/bin/talosctl\"</code></li> <li>Then you can use (or link to): <code>/home/linuxbrew/.linuxbrew/Cellar/talosctl/1.9.1/bin/talosctl</code></li> <li><code>alias talosctl='/home/linuxbrew/.linuxbrew/Homebrew/Cellar/talosctl/1.9.1/bin/talosctl --talosconfig=/home/colleymj/.talos/talosconfig'</code></li> </ul>"},{"location":"Talos/#pxe-boot","title":"PXE Boot","text":"<p>Use PXE for boot, with metadata service for per machine (MAC Address) configuration.</p> <pre><code>talos.config=https://metadata.service/talos/config?mac=${mac}\n</code></pre>"},{"location":"Talos/#talos-internet-proxy-support","title":"Talos internet proxy support","text":"<p>For initial boot, add command line arguments: <code>talos.environment=http_proxy=http://192.168.4.51:3128 talos.environment=https_proxy=http://192.168.4.51:3128</code></p> <p>Talos image factory specification, supporting drbd fir linode (existing), qemu-guest-agent for terraform remote control (existing), and a new squid proxy, on the cluster hardware.</p> <pre><code>customization:\n    extraKernelArgs:\n        - talos.environment=http_proxy=http://192.168.4.51:3128\n        - talos.environment=https_proxy=http://192.168.4.51:3128\n    systemExtensions:\n        officialExtensions:\n            - siderolabs/drbd\n            - siderolabs/qemu-guest-agent\n</code></pre> <p>For runtime, add machine configuration to the patch:</p> <pre><code>machine:\n  env:\n    http_proxy: http://192.168.4.51:3128\n    https_proxy: http://192.168.4.51:3128\n    no_proxy: \"localhost,127.0.0.1,192.168.4/24,10.244.0.0/16,10.96.0.0/12\"\n</code></pre>"},{"location":"Talos/#references","title":"References","text":"<ul> <li>https://www.talos.dev/v1.8/introduction/prodnotes/</li> <li>https://factory.talos.dev/</li> <li>https://www.talos.dev/v1.8/talos-guides/install/virtualized-platforms/proxmox/</li> <li>https://www.talos.dev/v1.8/talos-guides/install/cloud-platforms/nocloud/</li> <li>https://www.civo.com/blog/calico-vs-flannel-vs-cilium</li> <li>https://cilium.io/</li> <li>https://github.com/cilium/cilium</li> <li>KaaS GitOps: https://github.com/kubebn/talos-proxmox-kaas</li> <li>PXE: https://www.talos.dev/v1.8/talos-guides/install/bare-metal-platforms/pxe/</li> <li>Options: https://www.civo.com/blog/calico-vs-flannel-vs-cilium</li> </ul>"},{"location":"TalosTemplateCreation/","title":"Talos Template creation","text":"<p>This is a method of creating a VM template for use with Clauter API.</p> <p>There are arguably better methods, like scripting against the Proxmox API.</p> <p>This method is used because it is based on a pre Cluster API provisioning method that was used.</p>"},{"location":"TalosTemplateCreation/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>terraform installation</li> <li>Determine the Talos image required, and load it into the resource server.</li> <li>Tested with schematic ID: 6adc7e7fba27948460e2231e5272e88b85159da3f3db980551976bf9898ff64b</li> <li>Talos image config item: Nocloud</li> <li>Talos image config item: amd64</li> <li>Talos extensions: siderolabs/drbd, siderolabs/qemu-guest-agent</li> </ul> <p>Resulting customization specification:</p> <pre><code>customization:\n    systemExtensions:\n        officialExtensions:\n            - siderolabs/drbd\n            - siderolabs/qemu-guest-agent \n</code></pre> <p>Resulting image: https://factory.talos.dev/image/6adc7e7fba27948460e2231e5272e88b85159da3f3db980551976bf9898ff64b/v1.11.1/nocloud-amd64.iso</p> <p>Take note of the installation image for use in the Cluster API cluster template. In this case, it is ."},{"location":"TalosTemplateCreation/#usage","title":"Usage","text":"<ul> <li>Edit files.tf, correcting the talos image required. I use a local web server for image and manifests. The URL from Talos Image factory can be used directly.</li> <li>Edit providers.tf, correcting the details for the proxmox host</li> <li>Edit vm.tf, setting defaults for the the CPU, RAM and Disk</li> <li>Run <code>terraform init</code></li> <li>Run <code>terraform apply</code></li> <li>Once the VM is running, check that is is in maintenance mode.</li> <li>Stop the VM</li> <li>Convert the VM to a VM template</li> <li>Clone two VMs from the template, for the control plane and worker nodes of the management cluster</li> <li>Manually configure IP addresses in the VM cloudinit network configuration.</li> <li>Now delete the CloudInit drive from the template for user with Cluster API (Cluster API needs to create this on it's own)</li> <li>Note the Template VM ID, for use in the Cluster API configuration</li> </ul>"}]}